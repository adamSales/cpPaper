\documentclass[12pt]{article}

%\usepackage{endfloat}
%\usepackage{soul}

%\usepackage{type1ec}
\usepackage{subcaption}
\usepackage{fullpage}
\usepackage{enumerate}
%\usepackage{graphicx}
%\usepackage{graphics}
\usepackage{multirow}
\usepackage{comment}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{bbm}
\usepackage{setspace}
\usepackage{verbatim}
\usepackage{natbib}
\usepackage{bm}
\usepackage{pdflscape}
%\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage{xr}
%\usepackage{mathabx}
%\usepackage{filecontents}
%\usepackage{bibentry}
%\usepackage{hanging}
%\usepackage{apacite}
%\usepackage{hyperref}
\usepackage{makecell}

%\usepackage{endnotes}

%\let\footnote=\endnote


\title{Mastery Learning in Practice:\\
A Descriptive Analysis of Data from the  Cognitive Tutor Algebra I Effectiveness Trial}

\author{Anita Israni \and Adam C Sales \and John F Pane}


<<include=FALSE>>=
library(knitr)
#library(tikzDevice)
opts_chunk$set(
echo=FALSE, results='asis',cache=TRUE,warning=FALSE,error=FALSE,message=FALSE,autodep = FALSE
    )

#options(tikzDefaultEngine = "pdftex")
@

<<setup,include=FALSE>>=

library(lubridate)
library(scales)
library(reshape2)
library(dplyr)
library(ggplot2)
library(xtable)

addOverallState <- function(dat){
    levels(dat$state) <- c(levels(dat$state),'Overall')
    dat2 <- dat
    dat2$state <- 'Overall'
    dat2 <- rbind(dat,dat2)
    dat2$state <- factor(dat2$state, levels= levels(dat$state))
    dat2
}

@


<<data,include=FALSE>>=
if(file.info('cpPaper.RData')$mtime>file.info('dataMerge.r')$mtime){
    load('cpPaper.RData')
} else source('dataMerge.r')
@

<<data1,include=FALSE,cache=FALSE>>=
dataCache <- grep('data',list.files('cache'),ignore.case=FALSE,value=TRUE)[1]

if(file.info('dataMerge.r')$mtime>file.info(paste0('cache/',dataCache))$mtime){
    if(file.info('cpPaper.RData')$mtime>file.info('dataMerge.r')$mtime){
        load('cpPaper.RData')
    } else source('dataMerge.r')
}
@


\begin{document}
\maketitle

\section{Introduction}
Mastery learning sits at the foundation of intelligent tutoring systems. The philosophy of mastery learning assumes a well-structured curriculum, and posits that students progress within the curriculum as they master its skills. The Cognitive Tutor Algebra I (CTAI) system, developed by Carnegie Learning, Inc., is one of the best studied and best regarded examples of modern educational software. It is a blended learning system for teaching algebraic concepts and principles, to middle and high school students, including both textbook materials and software. The software component of the curriculum allows students to progress at their own pace and receive individualized feedback on their performance. A large-scale randomized effectiveness trial conducted by the RAND corporation showed that, in some circumstances, CTAI boosts students’ scores on an Algebra-I post-test by about one fifth of a standard deviation (Pane, et al.  2013). CTAI’s success in this experiment would seem to vindicate its pedagogy: mastery learning, and the algebra I curriculum on which it is based.

However, the theory underlying CTAI does not always determine its use. To be sure, the software has a standard set of algebra topics, divided into units and further into sections; and a standard sequence for presenting them. But this precise curriculum is not mandatory. At the request of educators, it can be customized by altering what units or sections are included (including, possibly, material from a different standard curriculum such as geometry), as well as their sequence, to conform to local or state standards. Further, teachers have the option of moving students within the curriculum, regardless of the software’s estimate of their skill mastery.

This article examines teachers’ and schools’ non-adherence to the standard, mastery-based CTAI curriculum, using data from the RAND study, a seven-state randomized controlled trial of CTAI in high schools and middle schools. The study found a significant positive effect of CTAI in high schools, during their second year of implementation but not the first. Students in the treatment group of the study were enrolled in one or more of the standard or customized curriculum during their participation in the study, and the software logged aspects of their usage, including time spent, sections encountered, and whether the software judged the students to have mastered the sections. Both Carnegie Learning and the researchers running the study restricted their software support and oversight to what is typically provided outside of an experimental context. Thus, the data from the study reflect typical software usage. Secondary data analyses used principal stratification to show that students who attempted more sections experienced larger treatment effects, and students who had high or low assistance levels, as opposed to an average level, experienced smaller treatment effects (Sales \& Pane, 2015, Sales et al. 2016).

Here we extend the previous findings to investigate patterns of change in the sequence of topics presented to students. As noted earlier, schools could use a standard curriculum, a customized curriculum, or switch from one curriculum to another. Each curriculum consisted of a sequence of units, which was comprised of a sequence of sections. For example, the standard CTAI Algebra I curriculum in Year 1 comprised of 40 units covering approximately 170 sections. The
number of sections within each unit varied.
This study attempts to elucidate teachers' goals in reassigning students. One hypothesis is the need for teachers push ahead students who were falling behind, i.e. to reassign them from sections on which they were struggling, to allow them catch up with the rest of the class. Another hypothesis is that teachers sought to push students past easier sections to begin working on more relevant topics for them. A third hypothesis is that teachers needed to cover certain topics in preparation for an upcoming state exams, and might have reassigned groups of students all at the same time to cover topics that might otherwise not have been covered. This hypothesis may lead to an increase in reassignments as the exam approaches. Overall, understanding patterns of reassignment is the focus of this paper, along with insights into how teachers were using the software component of CTAI.

\section{The RAND Effectiveness Trial}
The study to measure the effectiveness of CTAI included 7 states, 73 high schools, and 74 middle schools with nearly 18,700 high school students and 6,800 middle school students. Schools were enrolled in a total of 52 school districts that were distributed between urban, suburban, and rural areas. Schools were matched on a set of covariates, and then randomly assigned to the treatment or control group. Schools in the control group continued with their current algebra curriculum, and schools in the treatment group used Carnegie Learning’s curriculum which includes textbook materials as well as CTAI. Each school participated for two years, with a different cohort of students taking algebra the second year (with a small fraction of students present in the study both years because they repeated algebra). It should be noted that this study did not include statewide implementations; the study results cannot be generalized to all schools within the state. In some states, one large school district participated, while in other states, a set of smaller school districts participated. The states included Alabama (AL), Connecticut (CT), Kentucky (KY), Louisiana (LA), Michigan (MI), New Jersey (NJ), and Texas (TX). Each state participated in both the middle school and high school arms of the study, except AL, which participated only in the middle school arm. The current study focuses on high school students only, due to the large number of students in this cohort and the significant positive treatment effect previously found for this cohort. All discussion that follows refers exclusively to the high school data.

There are some limitations to the available data for this study. In particular, the developer was able to provide the intended sequence of units/sections for only a subset of the customized curricula. Data from some schools, and some students within schools, were missing either because the log files were not retrievable, or because of an imperfect ability to link log data to other study data files. In addition, there was missing completion dates for many customized curricula; making it difficult to analyze the customized curricula when any sort of date was needed, such as time to complete a section.

\section{Standard and Customized Curricula}

\begin{figure}
  \centering
<<curricula,dependson=c('setup','data'),fig.width=6.4,fig.height=3>>=

bbb <- addOverallState(data)%>%filter(!is.na(Curriculum) & State!='NJ')%>%
    group_by(state,Curriculum,overall,Yr)%>%
    summarize(n=n())
staten <- addOverallState(data)%>%filter(!is.na(Curriculum) & State!='NJ')%>%group_by(state,Yr)%>%summarize(n=n())

for(i in 1:nrow(bbb)) bbb$n[i] <- bbb$n[i]/staten$n[staten$state==bbb$state[i] & staten$Yr==bbb$Yr[i]]

COLS <- c('#1b9e77','#d95f02','#7570b3')

ggplot(bbb,aes(Yr,n,fill=Curriculum,alpha=overall))+geom_col()+facet_grid(~state)+scale_alpha_manual(values=c(1,0.5))+scale_fill_manual(values=COLS)+scale_y_continuous(labels=percent)+labs(x='',y='% of Problems Worked',alpha='')

## ggplot(bbb,aes(Yr,n,color=bbb))+geom_col(show.legend=FALSE)+geom_col(aes(fill=Curriculum))+geom_col(aes(fill=overall),alpha=0.5)+facet_grid(~state)
##     scale_fill_manual(values=c('#8dd3c7','#8dd3c7','#ffffb3','#ffffb3','#bebada','white','black'))
##     geom_col(aes(Yr,n,fill=overall),alpha=0.5)+
##     #geom_col(aes(Yr,n,fill=overall))+#scale_fill_manual(values=c('white','black'))+
##     facet_grid(~state)

## omar <- par()$oma

## par(oma=c(0,2,0,0),mar=c( 3,.5,2,0))
## par(mfrow=c(1,9))
## for(st in levels(data$State)){
## #    cwy <- currWorkedSt%>%filter(State==st)%>%group_by(Curriculum,overall)%>%summarize(`Yr 1`=p[Year=='Year 1'],`Yr 2`=p[Year=='Year 2'])
##     cwy <- data%>%filter(!is.na(Curriculum) & State==st)%>%group_by(Curriculum,overall)%>%
##     summarize(`Yr 1`=sum(Year=='Year 1'),`Yr 2`=sum(Year=='Year 2'))
##     for(y in c(1,2)) cwy[,paste('Yr',y)] <- cwy[,paste('Yr',y)]/sum(cwy[,paste('Yr',y)])
##     cwy <- cwy[nrow(cwy):1,]
##     barplot(as.matrix(cwy[,3:4]),col=COLS[as.numeric(cwy$Curriculum)],angle=ifelse(cwy$overall=='Customized',45,NA),density=ifelse(cwy$overall=='Customized',50,NA),main=st,yaxt='n')
##     if(st==levels(data$State)[1]) axis(2,at=seq(0,1,0.1),labels=paste0(seq(0,100,10),'\\%'))
##     #box()
## }
## cwy <- data%>%filter(!is.na(Curriculum))%>%group_by(Curriculum,overall)%>%
##     summarize(`Yr 1`=sum(Year=='Year 1'),`Yr 2`=sum(Year=='Year 2'))
## for(y in c(1,2)) cwy[,paste('Yr',y)] <- cwy[,paste('Yr',y)]/sum(cwy[,paste('Yr',y)])
## cwy <- cwy[nrow(cwy):1,]
## barplot(as.matrix(cwy[,3:4]),col=COLS[as.numeric(cwy$Curriculum)],angle=ifelse(cwy$overall=='Customized',45,NA),density=ifelse(cwy$overall=='Customized',50,NA),main='Overall',yaxt='n')

## plot(1:10,type='n',xaxt='n',yaxt='n',bty='n')
## legend(-1.5,7.5, c("Bridge-to-\n Algebra",'Algebra I','$>$Alg. I','','Standard','Cust.'),
##        fill=c(COLS,NA,'#010101','#010101'),border=rep('white',6),
##        angle=c(NA,NA,NA,NA,NA,45),density=c(NA,NA,NA,NA,NA,50),xpd = TRUE, bty = "n",inset=c(0,0))

## par(oma=omar)
@
\caption{Percentage of worked problems coming from various levels
  (denoted by color, with Algebra II and Geometry bundled as ``$>$Algebra I''), from
  standard and customized variants, denoted by shading.}
\label{fig:curricula}
\end{figure}

Students' automatic progress throught the Cognitive Tutor (CT) software is governed by
curricula---sequences of sections and units embedded in the software.
Without external meddling, the curriculum a student works on
determines what section he or she will be directed towards next after
mastering (or exhausting the problems) from a previous section.
In the CTAI effectiveness trial, the most common curriculum was,
naturally, Algebra I.
This came with three closely related varients, due to software releases.
Students requiring more remediation were able to work on a less
advanced curriculum, called ``Bridge to Algebra,'' and more advanced
students could work on Algebra II or Geometry.

In the second year of the study, some high schools, primarily in
Texas, Michigan, and Kentucky, requested customized variants of the
curricula.
This was typically due to state standards and testing schedules.
These ``customized curricula'' altered the order of some sections and
units, and were particular to schools.

Figure \ref{fig:curricula} shows the percentage of worked problems
from each curriculum, from standard and customized varieties, by state
and year.
First, note that the vast majority of worked problems were from the
Algebra I sequence.
A small but notable number of more advanced problems were worked in
Kentucky in Year 2, and some less advanced problems were worked in
Michigan and Louisiana.
Secondly, note the rise in ``customized curricula'' in year 2 in
Texas, Kentucky, and Michigan, the three states with the most students
in our dataset.
In particular, Texas shifted almost entirely to customized curriclua
from years 1 to 2.


% All students who participated in the treatment condition of the study were enrolled in at least one curriculum throughout the school year.
% Figure
% In Year 1, the standard Algebra curriculum was the most-used curriculum across schools participating in the study; other curricula were Bridge-to-Algebra (pre-algebra), Algebra II, and five customized curricula that were used in two schools. The standard curricula used in Year 2 included the three that were used in Year 1, plus Geometry and Algebra II Bonus. In Year 2, Carnegie Learning also developed many more customized curricula with input from participating schools, for a total of 33 curricula used in 13 schools. This led to fewer students enrolled in the standard curriculum and more enrolled in customized curricula overall in Year 2.

% In the available data, representing a subset of the entire study sample, the number of students enrolled in at least one standard curriculum in Year 1 was 2,801; in Year 2, this number decreased to 1,204. The number of students enrolled in at least one customized curriculum was 115 in Year 1 and increased to 2,333 in Year 2. It should be noted that students could be enrolled in multiple curricula. For instance, teachers could choose to enroll students in any of the standard curricula provided, such as Algebra I or Algebra II, along with any of the customized curricula that were used by the school.

% The use of customized curricula varied substantially by state. For instance, schools in CT and NJ did not use customized curricula in either Year 1 or Year 2. All other states contained schools that used customized curricula in addition to the standard curricula. CT schools maintained similar number of students in Algebra I across both years with 111 in Year 1 and 135 in Year 2. NJ substantially decreased the number of students in the CTAI program from 89 to 32 from Year 1 to Year 2. However, while all other states (KY, LA, MI, and TX) did decrease enrollment in the standard Algebra I curriculum the second year, those same states increased enrollment in the customized curricula. KY added 3 customized curricula with 146 students, LA added 15 customized curricula with 286 students, MI added 5 customized curricula with 370 students, and TX added 10 additional customized curricula and had the largest enrollment in these curricula with 1,531 students. Students enrolled in the standard Algebra I curriculum dropped from 974 in Year 1 to 61 in Year 2 in TX. As noted above, students could be enrolled in more than one curriculum. In Year 2, the average number of curricula per a student ranged from 1.00 (NJ and CT) to 2.53 (TX). The next highest value was LA at 2.10.

Throughout the school year, teachers could have a class of students
working on multiple curricula either sequentially, where the students
changed curricula in lock step, or simultaneously, where students
worked on different curricula at the same time. As an example, two
teachers located in KY had their students working on Algebra I
throughout most of the year and then reassigned them to Algebra II in
the last month of school. Another teacher in KY had students variously
enrolled in three different curricula throughout the entire year
(Bridge-to-Algebra, Algebra I, and a specialized Geometry curriculum),
while another teacher in MI during Year 2 had students enrolled in
three curricula sequentially throughout the year covering Algebra I
until November, and then starting a customized curriculum and using it
until February, and ending the year with another customized curriculum
that was used until June. While there are numerous instances of these
uses of multiple curricula in Year 2, there are also many occurrences
of teachers who had their students enrolled in the standard Algebra I
throughout the entire year.  In TX mostly, with some instances in MI,
there were teachers who exclusively used customized curricula the
whole year.

\section{Student Usage Across States and Years}

<<usageMedians,dependson=c('data','setup'),results='asis'>>=
secByStud <- data%>%group_by(state,Yr,field_id) %>% summarize(numSec=n_distinct(section,na.rm=TRUE),
                                                              numUnit=n_distinct(unit,na.rm=TRUE),
                                                              time=sum(total_t1,na.rm=TRUE),
                                                              mastered=n_distinct(section[status=='graduated'],na.rm=TRUE),
                                                              nprob=n_distinct(unit,section,Prob1,na.rm=TRUE))

secByStud$time <- secByStud$time/3600000

secByStud <- within(secByStud,{
    numUnit[numUnit==0] <- NA
    numSec[numSec==0] <- NA
    time[time==0] <- NA
    mastered[mastered==0] <- NA
})

secByStud2 <- addOverallState(secByStud)

secByStud2 <-
  secByStud2 %>%
  group_by(state,Yr) %>%
  mutate(outlier.time = time > quantile(time,0.75,na.rm=TRUE) + IQR(time,na.rm=TRUE) * 1.5,
         outlier.sec = numSec > quantile(numSec,0.75,na.rm=TRUE) + IQR(numSec,na.rm=TRUE) * 1.5,
         outlier.prob = nprob > quantile(nprob,0.75,na.rm=TRUE) + IQR(nprob,na.rm=TRUE) * 1.5,
         outlier.unit = numUnit > quantile(numUnit,0.75,na.rm=TRUE) + IQR(numUnit,na.rm=TRUE) * 1.5
         ) %>%
  ungroup


tab <- secByStud%>%group_by(Yr)%>%select(time,nprob,numSec,numUnit)%>%summarize_all(median,na.rm=TRUE)
tab$Yr <- NULL
tab <- as.data.frame(tab)
rownames(tab) <- c('Year 1','Year 2')
names(tab) <- c('Hours','Problems','Sections','Units')

xtable(tab,caption='Median numbers of hours, problems, sections, and units worked by each student in the dataset in the two years of the study. Students with no usage data were excluded.',label='tab:medUsage',digits=c(1,2,0,1,0))
@


Table \ref{tab:tab:medUsage} shows the median numbers of hours,
problems, sections, and units worked on by each student in the dataset
in the two years of the study.
Apparently, usage decreased markedly in the second year: the median
student worked about
\Sexpr{round(tab['Year 1','Hours']-tab['Year 2','Hours'])}
fewer hours,
\Sexpr{round(tab['Year 1','Problems']-tab['Year 2','Problems'])}
fewer problems, and
\Sexpr{round(tab['Year 1','Sections']-tab['Year 2','Sections'])} fewer
sections in year 2 than in year 1.


\begin{figure}
\centering
<<usageTime, fig.height=3,fig.width=6,dependson=c('data','setup','usageMedians')>>=


print(timeStateYear <- ggplot(filter(secByStud2,state!='NJ'),aes(Yr,time))+geom_boxplot(outlier.shape=NA)+
          geom_jitter(data = function(x) dplyr::filter_(x, ~ outlier.time), width=0.2)+
          facet_grid(~state)+coord_cartesian(ylim=c(0,110))+labs(x='',y='Hours on CT Software per Student'))
                                        #ggsave('timeStatYear.jpg',width=6,height=3)





@
\caption{Boxplots of hours each student spent on Cognitive Tutor
  software over by year and state. Students  with no timestamp data
  (\Sexpr{sum(is.na(secByStud[['time']]))}), with negative time
  (\Sexpr{sum(secByStud[['time']]<0,na.rm=TRUE)}) or with more than 110 hours (\Sexpr{sum(secByStud[['time']]>110,na.rm=TRUE)})
  were excluded.}
\label{fig:timeByStud}
\end{figure}

<<supplementalFigs1,include=FALSE,dependson=c('data','setup','usageMedians')>>=
secStateYear <- ggplot(filter(secByStud2,state!='NJ'),aes(Yr,numSec))+geom_boxplot(outlier.shape=NA)+
          geom_jitter(data = function(x) dplyr::filter_(x, ~ outlier.sec), width=0.2)+
          facet_grid(~state)+labs(x='',y='# of Sections Worked per Student')+coord_cartesian(ylim=c(0,200))
ggsave('secStateYear.jpg',width=6,height=3)

probStateYear <- ggplot(filter(secByStud2,state!='NJ'),aes(Yr,nprob))+geom_boxplot(outlier.shape=NA)+
          geom_jitter(data = function(x) dplyr::filter_(x, ~ outlier.prob), width=0.2)+
          facet_grid(~state)+labs(x='',y='# of Problems Worked per Student')+coord_cartesian(ylim=c(0,2000))
ggsave('probStateYear.jpg',width=6,height=3)

@

% \begin{figure}
% <<mastSec,dependson=c('usageTime','data','setup'),fig.width=6,fig.height=3>>=
% print(mastStateYear <- ggplot(secByStud2,aes(Yr,mastered))+geom_boxplot()+facet_grid(~state)+labs(x='',y='\\# Sections Mastered Per Student')+coord_cartesian(ylim=c(0,250)))
% @
% \caption{Boxplots of the number of sections each of Cognitive Tutor
%   software each student mastered, by year and state. Students
%   mastering more than 250 sections (\Sexpr{sum(secByStud[['mastered']]>250,na.rm=TRUE)/2}) of \Sexpr{nrow(secByStud)/2}) and students
%   with no mastery data (\Sexpr{sum(is.na(secByStud[['mastered']]))/2})
%   were excluded.}
% \label{fig:mastSec}
% \end{figure}



Figure \ref{fig:timeByStud} shows that the number of hours students
spent working on the CT software in some more detail, via
state-by-year boxplots.
Analogous figures for the numbers of problems and sections students
worked, available in an online supplement, showed similar patterns for
the numbers of sections and problems students worked.
Usage time varied substantially between students and across states and
years.
Time spent varied substantially by state, with students in Texas,
Connecticut, and New Jersey working far fewer hours than students in
Kentucky, Louisiana, and Michigan.
Not every state reduced its usage from years 1 to 2---while students
in Texas, Kentucky and New Jersey used the software less in the second
year than in the first, students in Michigan, Louisiana, and
Connectuct increased their usage.

Overall, usage varied a bit more in year 2 than in year
1---the median absolute deviation of time spent was \Sexpr{round(mad(secByStud[['time']][secByStud[['Yr']]=='Yr 1'],na.rm=TRUE,constant=1),1)} hours in the first year, compared to \Sexpr{round(mad(secByStud[['time']][secByStud[['Yr']]=='Yr 2'],na.rm=TRUE,constant=1),1)} in  the second year.
The increase in variation seems to be driven both by increasing
between-state variation, and a between-student increase in Louisiana.
One intriguing possibility is that CT usage was better tailored to
teachers and students in the second year than in the first---perhaps
students who stood to gain more from the software increased their
usage, and the rest of the students used it less.

In contrast to the numbers of hours, problems, and sections students
worked, Table \ref{tab:medUsage} shows that the median number of units
students worked increased by
\Sexpr{round(tab['Year 2','Units']-tab['Year 1','Units'])}
from year 1 to year 2.
This suggests students in year 2 were exposed, on average, to a
slightly wider range of topics.
Figure \ref{fig:unitsByStud} shows boxplots of the numbers of units
worked by state and year.
The geographic variation in units worked mirrors the pattern in Figure
\ref{fig:timeByStud}, with more usage in Kentudky, Michigan, and
Louisiana but less in Texas, Connecticut, and New Jersey.
However, in every state the median Year 2 student worked at least as
many different units as the median Year 1 student.
Variation in the number of units worked also increased slightly
from years 1 to 2---the interquartile range increased in every state
except for Kentucky, where a decrease in IQR was accompanied by an
incrase in the number of outliers.



\begin{figure}
\centering
<<unitsWorked,dependson=c('usageMedians','data','setup'),fig.width=6,fig.height=3>>=
secByStud2 <-
  secByStud2 %>%
  group_by(state,Yr) %>%
  mutate(outlier = numUnit > quantile(numUnit,0.75,na.rm=TRUE) + IQR(numUnit,na.rm=TRUE) * 1.5) %>%
  ungroup

print(unitsStateYear <- ggplot(filter(secByStud2,state!='NJ'),aes(Yr,numUnit))+geom_boxplot(outlier.shape=NA)+geom_jitter(data = function(x) dplyr::filter_(x, ~ outlier), width=0.2)+facet_grid(~state)+labs(x='',y='# Units Worked Per Student')+coord_cartesian(ylim=c(0,55)))
@
\caption{Boxplots of the number of units of Cognitive Tutor
  software each student worked, by year and state. Students
  working more than 55 units (
  \Sexpr{sum(secByStud[['numUnit']]>55,na.rm=TRUE)} of
  \Sexpr{nrow(secByStud)}) and students
  with no usage data (\Sexpr{sum(is.na(secByStud[['numUnit']]))})
  were excluded.}
\label{fig:unitsByStud}
\end{figure}

All in all, students used CT software less in year 2 than in year
1.
On the other hand, students in the second year tended to see a
slightly wider range of topics, and varied somewhat more in their usage.

\begin{comment}
Of course, more worked sections does not necessarily mean better worked sections. For instance, students might attempt fewer sections in year two, but graduate a higher percentage. Thus we investigated whether, on average, there were more graduated sections per students the second year, and how this related to whether they were enrolled in the standard versus customized curricula. Overall, the number of graduated sections per student increased in Year 2. The number of graduated sections per student was 17.94 in Year 1 and 25.66 in Year 2 in standard curricula. However, the customized curricula had fewer average graduated sections per student enrolled compared to the standard curricula; these numbers were 5.64 in Year 1 and 11.47 in Year 2. The percentages of average graduated sections to the average total worked sections were: 33.33% in Year 1 and 48.92% in Year 2 in the standard curricula, and 57.55% in Year 2 for the customized curricula. The average number of promoted sections per student also increased from Year 1 to Year 2, though only slightly. In Year 1, the average number of promotions were 1.20 for the customized curricula versus 3.64 for the standard curricula; in Year 2, those numbers were 2.20 and 4.95, respectively.

We also evaluated the time spent per section across different curricula and states.  Some records were omitted from this analysis: sections missing information on date of completion, as well as each student’s last section attempted because there is no completion date recorded for those sections. As presented in Figures 2 and 3 for Year 1 and 2, respectively, students in NJ increased the time spent per section from Year 1 to Year 2; however, there were relatively few students in the study from NJ. From Figure 1, it was shown that CT and TX completed, on average, fewer sections per student. From the information presented in Figures 2 and 3, these states spent the highest average number of days per section. TX, with the largest enrollment of students and the greatest use of customized curricula in Year 2, had students spending slightly more days completing sections in customized curricula relative to the standard curricula.

Figures 4 and 5 present information for only sections that were “graduated” by students, that is, cases in which a student was judged by the software to have mastered the material in the section. Results are similar to the data presented in Figures 2 and 3. However, in Year 2, TX students in customized curricula spent fewer days on a section they mastered, compared to the time TX students spent mastering sections in the standard curricula. This difference is also seen in KY and LA. If customized curriculum are more aligned with teachers’ instruction, graduating from a section may require fewer days than sections in the standard curricula, possibly because students are working on the same topics in class as they are in the software. Figures 4 and 5 also show that, on average, CT and TX spent more days graduating from sections, regardless whether the section was part of a standard curricula or customized curricula. One explanation for the longer time periods in these states may be due to a teacher’s use of the software simultaneously with the textbook instruction. In this case, the teachers could have switched back-and-forth as they instructed, hence extending the time worked by their students. For the states with shorter time periods, teachers may have followed a more sequential process where they first completed one component, e.g. textbook material, and then allowed students to focus on and complete the second component, e.g. software.
\end{comment}


\section{Working Units in Order---Or Not}

Overally, students used CT less in the second year than in the
first.
How was this difference distributed between CTAI units?

\begin{figure}
<<whichUnits,fig.height=4,fig.width=6,dependson=c('data','setup')>>=

curricula <- read.csv('~/Box Sync/CT/data/sectionLevelUsageData/RAND_study_curricula.csv',stringsAsFactors=FALSE)
curricula <- subset(curricula,curriculum_name=='algebra i')
curricula$unit <- tolower(curricula$unit)
curricula <- subset(curricula,unit%in%intersect(curricula$unit[curricula$ct=='2007'],curricula$unit[curricula$ct=='2008r1']))
units <- curricula$unit[curricula$ct=='2007']
sectionStats <- read.csv('~/Box Sync/CT/data/sectionLevelUsageData/section_stats_withAbb.csv',stringsAsFactors=FALSE)
UnitName <- sectionStats$unit_name_abb[match(units,sectionStats$unit_id)]
UnitName[units=='inequality-systems-solving'] <- 'Systems of Lin. Ineq.'
UnitName[units=='intro-pythag-theorem'] <- 'Pythagorean Theorem'
UnitName[units=='linear-inequality-graphing'] <- 'Graphs of Lin. Ineq.'
UnitName[units=='linear-systems-solving'] <- 'Systems of Lin. Eq. Solving'
UnitName[units=='probability'] <- 'Probability'
UnitName[units=='unit-conversions'] <- 'Unit Conversions'

nstud <- data%>%filter(!is.na(unit))%>%group_by(Year)%>%summarize(nstud=n_distinct(field_id))
data$Unit <- data$unit
data$Unit[grep('unit-conversions',data$Unit)] <- 'unit-conversions'

unitLevel <- data%>%filter(Unit%in%units)%>%group_by(Unit,Year)%>%summarize(numWorked= n_distinct(field_id,na.rm=TRUE),numCP=sum(status=='changed placement',na.rm=TRUE),meanCP=mean(status=='changed placement',na.rm=TRUE))

unitLevel$perWorked <- unitLevel$numWorked/nstud$nstud[match(unitLevel$Year,nstud$Year)]

unitLevel$Unit <- factor(unitLevel$Unit,levels=units)
levels(unitLevel$Unit) <- UnitName

unitLevel$year <- factor(ifelse(unitLevel$Year=='Year 1',1,2))
print(unitsWorked <- ggplot(unitLevel,aes(x=Unit,y=perWorked,color=year,group=year))+geom_point()+geom_line()+theme(axis.text.x = element_text(angle = 90, hjust = 1,vjust=0.5,size=10),legend.text=element_text(size=10),legend.position=c(.96,.82))+labs(x='',y='% Worked',color='Year')+scale_y_continuous(labels=percent))

@
\caption{The percentages of students with usage data who worked at
  least one problem from each unit in the Algebra I curriculum. The
  units are arranged in order for the standard curriculum. In the 2008
version of the software, the ``Unit Conversions'' unit was broken up
into two smaller units; for the sake of between-year comparisons, we
re-combined them.}
\label{fig:unitsWorked}
\end{figure}


Figure \ref{fig:unitsWorked} shows the percentage of students with
usage data who worked each section from the standard CTAI curriculum.
The units are ordered, from left to right, as the are prescribed in
the standard CTAI curriculum.

In year 1, the curve is almost monontonically decreasing, as one would
expect if students adhered to the curriculum.
Students varied in how many units they worked---with the variation due
to both student ability and the amount of time allocated to CTAI
within a classroom---but they mostly followed the same curriculum.
Students who worked fewer units stopped earlier in the curricular
order, and those who worked more units progressed farther.
Hence, earlier sections were worked by higher proportions of students
than later units.

In contrast, in year 2 students were much more likely to buck the
standard unit order.
For instance, Figure \ref{fig:unitsWorked} suggests that students
skipped ``Unit Conversions'' and ``1 step Lin. Eq.'' for ``1st
Quadrent Linear Graphs'' and ``Ind. Variables in Lin. Mod.''
Both of the latter sections were worked by a greater proportion of
students than the former sections that preceded them in the CTAI
curriculum.
Most strikingly, ``Lin. Eq. w/ Var's Both Sides'' was worked by a
greater proportion of students in Year 2 than in Year 1, and by a
greater proportion of students than any of the previous six sections.
Presumeably teachers and administrators wanted students to focus on
that unit, perhaps because they found it to be particularly effective,
because students tend to struggle with its main topic, or because its
topic may figure prominantly in an upcoming standardized test.

\begin{figure}
  \centering
<<unitsWorkedCust,dependson=c('data','setup','whichUnits'),fig.height=4,fig.width=6>>=
### by customized curriculum (at school level)
cust <- data%>%filter(Year=='Year 2')%>%group_by(schoolid2,state)%>%summarize(cust=mean(overall=='Customized',na.rm=T))%>%arrange(cust)
data$cust <- ifelse(data$schoolid2%in%cust$schoolid2[cust$cust>0.8],'Customized','Standard')

nstudCust <- data%>%filter(!is.na(unit) & Year=='Year 2')%>%group_by(cust)%>%summarize(nstud=n_distinct(field_id))
unitLevelCust <- data%>%filter(Unit%in%units & Year=='Year 2' )%>%group_by(Unit,cust)%>%summarize(numWorked= n_distinct(field_id,na.rm=TRUE),numCP=sum(status=='changed placement',na.rm=TRUE),meanCP=mean(status=='changed placement',na.rm=TRUE))

unitLevelCust$perWorked <- unitLevelCust$numWorked/nstudCust$nstud[match(unitLevelCust$cust,nstudCust$cust)]

unitLevelCust$Unit <- factor(unitLevelCust$Unit,levels=units)
levels(unitLevelCust$Unit) <- UnitName
ggplot(unitLevelCust,aes(x=Unit,y=perWorked,color=cust,group=cust))+geom_point()+geom_line()+theme(axis.text.x = element_text(angle = 90, hjust = 1,vjust=0.5,size=10),legend.text=element_text(size=10),legend.position=c(.96,.82))+labs(x='',y='% Worked',color='Curriculum',title='Year 2')+scale_y_continuous(labels=percent)
@
\caption{The percentages of year-2 students with usage data who worked at
  least one problem from each unit in the Algebra I curriculum. The
  units are arranged in order for the standard curriculum. Students
  are divided between those attending schools using primarily a customized
  curriculum and those using primariy the standard Algebra I curriculum.}
\label{fig:unitsWorkedCust}
\end{figure}

Most of the variation in unit order was driven by the rise, in year 2,
of customized curricula.
Figure \ref{fig:unitsWorkedCust} divides year-2 students into those
attending schools using primarily a customized curriculum, and those
attending schools using primarily a standardized
curriculum.\footnote{At least
  \Sexpr{trunc(100*min(cust[['cust']][cust[['cust']]>0.8]))}\% of
  problems at ``Customized'' schools were from a customized curriculum,
  and at most
  \Sexpr{ceiling(100*max(cust[['cust']][cust[['cust']]<0.8]))}\% of
  problems at ``Standard'' schools were from a customized curriculum.}
Students using a standardized curriculum followed the standard
sequence---more or less---while students using customized curricula
did not.
That said, there were some order violations in the standard group:
specifically, more students worked problems from units ``4-Quadrant
Linear Graphs'' and ``Exponents'' than worked the preceding sections;
this suggests that some teachers used the reassignment tool to
prioritize particular topics.
Of course, this may have occurred in schools with customized
curriculua as well---a possibility we will discuss in the next
section.

\begin{figure}
  \centering
<<unitsBySchool,dependson=c('data','setup','whichUnits'),fig.height=7,fig.width=6>>=
### by school
nstudSch <- data%>%filter(!is.na(unit) & Year=='Year 2')%>%group_by(schoolid2,state)%>%summarize(nstud=n_distinct(field_id))
unitLevelSch <- with(filter(data,Unit%in%units & Year=='Year 2'),expand.grid(Unit=unique(Unit),schoolid2=unique(schoolid2)))
unitLevelSch$state <- data$state[match(unitLevelSch$schoolid2,data$schoolid2)]
unitLevelSch$cust <- data$cust[match(unitLevelSch$schoolid2,data$schoolid2)]
unitLevelSch$numWorked <- with(filter(data,Year=='Year 2'),vapply(1:nrow(unitLevelSch),function(i)
    n_distinct(field_id[schoolid2==unitLevelSch$schoolid2[i] & Unit==unitLevelSch$Unit[i]]),1))

#unitLevelSch <- data%>%filter(Unit%in%units & Year=='Year 2')%>%group_by(Unit,schoolid2,cust,state)%>%summarize(numWorked= n_distinct(field_id,na.rm=TRUE),numCP=sum(status=='changed placement',na.rm=TRUE),meanCP=mean(status=='changed placement',na.rm=TRUE))

unitLevelSch$perWorked <- unitLevelSch$numWorked/nstudSch$nstud[match(unitLevelSch$schoolid2,nstudSch$schoolid2)]

unitLevelSch$Unit <- factor(unitLevelSch$Unit,levels=units)
levels(unitLevelSch$Unit) <- UnitName

ggplot(filter(unitLevelSch,state!='NJ'),aes(x=Unit,y=perWorked,color=schoolid2,group=schoolid2,linetype=cust))+geom_point(size=.5)+geom_line()+theme(axis.text.x = element_text(angle = 90, hjust = 1,vjust=0.5,size=10),legend.text=element_text(size=10))+
                                        #,legend.position=c(.96,.82))+
    facet_grid(state~.)+scale_y_continuous(labels=percent)+scale_color_discrete(guide=FALSE)+
    labs(x='',y='% Worked',linetype='Curriculum',title='Year 2')
@
\caption{The percentages of year-2 students with usage data in each
  school who worked at least one problem from each unit in the Algebra I curriculum. The
  units are arranged in order for the standard curriculum. Schools are
  classified as either using primarily customized curricula (solid
  line) or using primarily the standard Algebra I curriculum (dotted).}
\label{fig:unitsBySchool}
\end{figure}

Figure \ref{fig:unitsBySchool} further decomposes the year-2 results
by school and state, showing a large amount of variation between
states, as well as variation between schools within states.
In Texas, every school used customized curricula, most of which seem to
prioritize some of the same units, for instance, ``Linear Patterns,''
``Ind. Variables in Lin. Mod.,'' and
``Lin. Eq. w/ Var's Both Side.''
These topics might be emphasized on Texas standardized tests.
On the other hand, there was also variance between schools.
For instance, one school prioritized units ``2 Step Lin. Eq.''
and ``4−Quadrant Linear Graphs'' while nearly eliminating ``Linear Patterns.''

Between-school variation is evident in the other states, as well.
In four of the five Kentucky schools, nearly every student worked on
the first nine units; in the one Kentucky school that used a
customized curriculum, nearly every student worked on the first 13
units, omitted the 15th (``Lin. Mod. in General Form''), and worked on
the 16th and 17th (``Literal Eq.'' and ``Lin. Eq. w/ Var's Both Sides'').
In the remaining school, nearly every student worked on the first
section, but usage decreased rapidly from there.
Curiously, in one Michigan school which used the standard curriculum,
no students seem to have worked on the ``Lin. Mod. \& Ratios'' section.

If unit order and topic scaffolding are important to CT's mastery
learning mechanism, the wide variation in students' realized curricula
would seem to pose a problem.
The fact that the prescribed order was followed less in the second
year of the study, when CTAI was effective, than in the first year,
when it wasn't, suggests that the standard curriculum may play a
smaller role than one might otherwise imagine.

\section{Mastering the Material---Or Not}
The central idea behind mastery learning is that students progress
through the curriculum as they master skills.
In the context of CT, skills are clustered within sections, which are
in turn clustered within units.
Students progress from the current section to the next section after
mastering all of the current section's skills.
Ideally, students would master all of the skills in all of the
sections they work.

By default, the software operates by automatically moving students
from section to section based on the sequence of topics defined by the
curriculum they were currently enrolled in. In this
software-controlled sequencing, students ideally spend the time
necessary to learn the material of a section, are judged by the
software to have mastered the material, and then ``graduate'' to the
next section. Students who exhaust a section's material without
mastering its skills are “promoted” to the next section.
Students left uninterrupted follow the sequence of topics defined by
their curriculum, ideally graduating from topics by showing mastery.
However, teachers could modify a student’s path within the curriculum.
They could ``reassign'' students from their current sections to other
sections earlier or later in the intended sequence, including sections
they had worked on previously.
Finally, if the semester ends, or a student stops using CT for some
other reason, while in the middle of working through a
section, he or she will leave their ``Final'' section without
mastering all of its skills.
All in all, each CT section begun ends in one of four possible ways:
mastery, promotion, reassignment, or as the student's final section.

\begin{figure}
  \centering
<<overallStatus,dependson=c('data','setup'),fig.height=3,fig.width=6>>=
statusPerSec <- data%>%
    filter(!is.na(Curriculum) &!(state=='MI'&Curriculum=='Customized'&year==1))%>%
    group_by(field_id,Yr,state,section,Curriculum,status)%>%
        summarize(cp=any(status=='changed placement'))%>%group_by(state,Curriculum,Yr,field_id)%>%summarise(pcp=mean(cp,na.rm=TRUE),ncp=sum(cp,na.rm=TRUE))

statusOverall <-  data%>%filter(!is.na(status))%>%
    group_by(field_id,Yr,state,unit,section,Curriculum,overall)%>%summarize(status=max(status))

statusStateYr <- addOverallState(statusOverall) %>% group_by(state,Yr)%>%
    summarize(pgrad=mean(status=='graduated'),pfoi=mean(status=='final_or_incomplete'),
              pcp=mean(status=='changed placement'),pprom=mean(status=='promoted'))%>%
    melt(measure.vars=c('pgrad','pfoi','pcp','pprom'))

levels(statusStateYr$variable) <- list(Final='pfoi',Reassigned='pcp',Promoted='pprom',Mastered='pgrad')


 ggplot(filter(statusStateYr,state!='NJ'),aes(Yr,value,fill=variable))+geom_col()+facet_grid(~state)+labs(y='% of Sections Worked',x='',fill='Exit Status')+scale_y_continuous(labels=percent)

@
\caption{The distributions of outcomes of worked sections, by state and across the
  entire sample, in the two study years.}
\label{fig:overallStatus}
\end{figure}

\begin{table}
 \begin{tabular}{rllllll|llllll}

&   \multicolumn{6}{c}{Year 1}&\multicolumn{6}{c}{Year 2}\\
<<statusTab,dependson='overallStatus',results='asis'>>=
statusStateYr2 <- addOverallState(statusOverall)
statusStateYr2a <- statusStateYr2%>%group_by(state,Yr)%>%
    summarize(pgrad=sum(status=='graduated'),pfoi=sum(status=='final_or_incomplete'),
              pcp=sum(status=='changed placement'),pprom=sum(status=='promoted'))%>%
    melt(measure.vars=c('pgrad','pfoi','pcp','pprom'))
statusStateYr2b <- statusStateYr2%>%group_by(state,Yr)%>%
    summarize(pgrad=mean(status=='graduated'),pfoi=mean(status=='final_or_incomplete'),
              pcp=mean(status=='changed placement'),pprom=mean(status=='promoted'))%>%
    melt(measure.vars=c('pgrad','pfoi','pcp','pprom'))


levels(statusStateYr2a$variable) <- list(Final='pfoi',Reassigned='pcp',Promoted='pprom',Mastered='pgrad')
levels(statusStateYr2b$variable) <- list(Final='pfoi',Reassigned='pcp',Promoted='pprom',Mastered='pgrad')

tab1 <- dcast(subset(statusStateYr2a,Yr=='Yr 1'&state!='NJ'),variable~state)
tab2 <- dcast(subset(statusStateYr2a,Yr=='Yr 2'&state!='NJ'),variable~state)
tab <- cbind(tab1,tab2[-1])

tab1b <- dcast(subset(statusStateYr2b,Yr=='Yr 1'&state!='NJ'),variable~state)
tab2b <- dcast(subset(statusStateYr2b,Yr=='Yr 2'&state!='NJ'),variable~state)


tab$variable <- as.character(tab$variable)
cat('&')
cat(names(tab)[-1],sep='&')
cat('\\\\')
for(i in 1:nrow(tab)){
    cat(tab[i,1],round(unlist(tab[i,-1])),sep='&')
    cat('\\\\')
}

rownames(tab1) <- rownames(tab2) <- tab1$variable
tab1 <- as.matrix(tab1[,-1])
tab2 <- as.matrix(tab2[,-1])

rownames(tab1b) <- rownames(tab2b) <- tab1b$variable
tab1b <- as.matrix(tab1b[,-1])
tab2b <- as.matrix(tab2b[,-1])



@

\end{tabular}
\caption{Percentages of worked sections that ended in each of the
  four possible outcomes, across states and study years.}
\label{tab:overallStatus}
\end{table}

Figure \ref{fig:overallStatus} and Table \ref{tab:overallStatus} show
the proportions of worked sections in each state and study year that
ended with mastery, promotion, or reassignment, or which were the
student's final section.
In the first year, about
\Sexpr{round(mean(tab1b['Mastered',c('TX','KY','MI','LA')])*100)}\% of
worked sections are mastered in every
state other than Connecticut.
Other than in Texas, about
\Sexpr{paste(round(range(tab1b['Promoted',c('CT','KY','MI','LA')])*100),collapse='--')}\%
of sections end in promotion.
About \Sexpr{round(tab1b['Reassigned','TX']*100)}
and \Sexpr{round(tab1b['Reassigned','CT']*100)}\% of sections in Texas
and Connecticut end in reassignment, which is even rarer in the other states.

With the exception of Texas, sections in the second year tend to be
completed as they were in year 1.
In Texas, however, the percentage of sections ending in reassignment increased by a factor
of about about
\Sexpr{round(tab2b['Reassigned','TX']/tab1b['Reassigned','TX'])},
to about \Sexpr{round(tab2b['Reassigned','TX']*100)}\%.
The proportion of Texas sections labeled ``Final'' increased as well,
most likely due to the decrease in usage.

Across states, just under
\Sexpr{round(tab1b['Reassigned','Overall']*100)}\% of sections in year
1 eneded in reassignment, as did roughly
\Sexpr{round(tab2b['Reassigned','Overall']*100)}\% in year 2.

\subsection{Section Mastery and Curriculum}
\begin{figure}
<<statusCur,dependson='overallStatus',fig.height=3,fig.width=6>>=
statusOverall$Curr2 <- with(statusOverall,
                        ifelse(Curriculum=='Algebra I',
                                     ifelse(overall=='Standard','Algebra I','Algebra I (Cust.)'),
                                     as.character(Curriculum)))
statusCurr <- statusOverall%>%filter(!is.na(Curriculum))%>% group_by(Curr2,Yr)%>%
    summarize(pgrad=mean(status=='graduated'),pfoi=mean(status=='final_or_incomplete'),
              pcp=mean(status=='changed placement'),pprom=mean(status=='promoted'))%>%
    melt(measure.vars=c('pgrad','pfoi','pcp','pprom'))
levels(statusCurr$variable) <- list(Final='pfoi',Reassigned='pcp',Promoted='pprom',Graduated='pgrad')
statusCurr$Curr2 <- factor(statusCurr$Curr2,levels=c('Bridge-to-Algebra','Algebra I','Algebra I (Cust.)','>Algebra I'))
print(ggplot(statusCurr,aes(Yr,value,fill=variable))+geom_col()+facet_grid(~Curr2)+
      labs(y='% of Sections Worked',x='',fill='Exit Status')+scale_y_continuous(labels=percent))

@
\caption{The distributions of outcomes of worked sections, by
  curriculum, in the two study years. (There were no Bridge-to-Algebra
sections in customized curricula in our dataset.)}
\label{fig:statusCur}
\end{figure}

A well-designed curriculum, can, in theory, play an important role in
students' attainment of mastery.
Students who work on appropriate problems that build on their current
set of skills should be more likely to master new skills than students
working on problems above their level.
What role did curriculum play in mastery during the CTAI effectiveness trial?

Figure \ref{fig:statusCur} shows the proportions of worked sections
that were mastered or ended in promotion, reassignment, or finality, in
standard and customized versions of each CT curriculum.
Mastery proportions do, indeed, depend on curriculum.
Specifically, students mastered sections from more advanced curriculua
less frequently.
Sections from the most basic curriculum, Bridge to Algebra, were
mastered
\Sexpr{round(statusCurr[['value']][statusCurr[['Curriculum']]=='Bridge-to-Algebra'
  & statusCurr[['variable']]=='Graduated']*100)}\% of the time;
those from Algebra I were mastered
\Sexpr{round(
  mean(statusOverall$status[statusOverall$Curriculum=='Algebra I']=='graduated',na.rm=T)*100)}\%
of the time, and those from more advanced curriculua were mastered
at a rate of
\Sexpr{round(
  mean(statusOverall$status[statusOverall$Curriculum=='>Algebra I']=='graduated',na.rm=T)*100)}\%.
This is unsurprising, since more advanced curriculua may be expected
to be more challenging.
However, it may suggest that some students studying advanced topics
would fair better in more standard curricula.

Algebra I sections from customized curricula tended to end in
reassignment more often than sections from the standard Algebra I
curriculum (\Sexpr{round(100*subset(statusCurr,Curr2=='Algebra I (Cust.)' &
  variable=='Reassigned')[['value']])}\% vs.
\Sexpr{round(100*subset(statusCurr,Curr2=='Algebra I' & variable=='Reassigned')[['value']])}\%).
This may an overall skepticism towards the Carnegie Learning standards
among certain schools and teachers.

% A somewhat lower proportion of Algebra I sections from customized
% curricula were mastered
% (\Sexpr{round(100*subset(statusCurr,Curr2=='Algebra I (Cust.)' & variable=='Graduated')[['value']])}\%)
% than from the standard Algebra I curriculum
% (\Sexpr{round(100*subset(statusCurr,Curr2=='Algebra I' & variable=='Graduated')[['value']])}\%).
% This difference must be interpreted with caution, since a greater
% proportion of sections of the customized curriculum ended in
% reassignment

% or were final
% (\Sexpr{round(100*subset(statusCurr,Curr2=='Algebra I (Cust.)' &   variable=='Final')[['value']])}\% vs.
% \Sexpr{round(100*subset(statusCurr,Curr2=='Algebra I' & variable=='Final')[['value']])}\%)
% , as well.
% The proportion of sections ending in promotion was almost identical:
% (\Sexpr{round(100*subset(statusCurr,Curr2=='Algebra I (Cust.)' &variable=='Promoted')[['value']])}\% vs.
% \Sexpr{round(100*subset(statusCurr,Curr2=='Algebra I' & variable=='Promoted')[['value']])}\%).


\section{Digging Deeper into Section Reassignment}
The proportion of worked sections in our dataset ending in
reassignment was small.
Nevertheless, since reassignement represents the only mechanism by
which individual teachers can affect their students' progress through
the Cognitive Tutor, exploring patterns of reassignment can provide
insight into how CT was used.

<<cpDat,dependson=c('data','setup'),include=FALSE>>=
secLev <- data%>%filter(is.finite(status) & is.finite(timestamp) &is.finite(date))%>%
    group_by(field_id,unit,section,Year,Yr,classid2,schoolid2)%>%
    summarize(startDate=min(date),endDate=max(date),startTime=min(timestamp),endTime=max(timestamp),state=state[1],
              status=max(status),Curriculum=Curriculum[1],overall=overall[1],version=version[1])%>%
    arrange(endDate)

### cp over time
secLev <- within(secLev,endMonth <- factor(month(secLev$endDate,TRUE,TRUE),levels=c('Aug','Sep','Oct','Nov','Dec','Jan','Feb','Mar','Apr','May','Jun','Jul')))

@

\subsection{How Do Reassignment Patterns Vary?}

\begin{figure}
<<vcs,fig.height=3,fig.width=6.5,dependson=c('data','setup')>>=
load('vcMods.RData')
library(lme4)

vcFun <- function(nm){
    mod <- vcMods[[nm]]
    out <- unlist(summary(mod)$varcor)
    out <- data.frame(sig2=out,comp=names(out),stringsAsFactors=FALSE)
    out <- rbind(out,data.frame(sig2=pi^2/3,comp='resid'))
    out$state <- strsplit(nm,'_')[[1]][1]
    out$year <- strsplit(nm,'_')[[1]][2]
    out$sig2 <- out$sig2/sum(out$sig2)
    out
}

vcDat <- do.call('rbind',lapply(names(vcMods),vcFun))

yr1 <- data.frame(sig2=c(unlist(summary(vcModYr[[1]])$varcor),pi^2/3),
                          comp=c(names(summary(vcModYr[[1]])$varcor),'resid'),
                          state='Overall',
                  year='1')
yr1$sig2 <- yr1$sig2/sum(yr1$sig2)

yr2 <- data.frame(sig2=c(unlist(summary(vcModYr[[2]])$varcor),pi^2/3),
                          comp=c(names(summary(vcModYr[[2]])$varcor),'resid'),
                          state='Overall',
                  year='2')
yr2$sig2 <- yr2$sig2/sum(yr2$sig2)

vcDat <- rbind(vcDat,yr1,yr2)

vcDat$comp <- factor(vcDat$comp)
levels(vcDat$comp)=list(State='state',School='schoolid2',Class='classid2',Student='field_id',Unit='unit',Residual='resid')


vcDat$state <- factor(vcDat$state,levels=c('TX','KY','MI','Overall'))

vcs <- list()
for(s in unique(vcDat$state)) for(y in 1:2){
 ddd <- round(subset(vcDat,state==s&year==y,select=c(sig2))*100)
 rownames(ddd) <- gsub('\\d','',rownames(ddd))
 rownames(ddd)[nrow(ddd)] <- 'resid'
 vcs[[paste0(s,'_',y)]] <- ddd
}


ggplot(vcDat,aes(year,sig2,fill=comp))+geom_col()+facet_grid(~state)+scale_fill_manual(values=rev(c('white','grey','#e41a1c','#377eb8','#4daf4a','#984ea3')))+labs(x='',y='% Variance Explained',fill='',title='Model: Multilevel Logistic Unconditional')+scale_y_continuous(labels=percent)



@
\caption{Results from a set of eight multilevel logistic regressions
  predicting section reassignment. For each year, in the entire sample (``Overall'')
  and in the three states with the largest numbers of reassignments (Texas,
  Kentucky, and Michigan), we regressed a binary variable indicating
  whether a section ended in reassignment on random intercepts for
  school, class, student, and unit, and in the overall case, for state
  as well, and recorded their variance. The residual variance was set
  as the variance of the standard logistic distribution,
  $\pi/3$. These bar charts give the proportion of the total variance
  attributable to each random effect.}
\label{fig:vc}
\end{figure}
Teachers alone control reassignment.
Nevertheless, the factors influencing student reassignment vary at a
number of levels.
For instance, state and district standards may prod teachers into
reassigning students to particular units.
Some prinicpals may encourage teachers to adhere to the official
curriculum and avoid reassignment.
Some students may be more prone to reassignment than others.
Certain units in the CTAI curriculum may be harder than others,
causing students to tarry and teachers to reassign.
Finally, a host of other factors, at these levels and others, may spur reassignment.

To better understand the source of the variation in
reassignment---what drives some, but not other, sections to end in
reassignment---we fit a set of multilevel models.
We fit separate models to data from each the three states with the highest numbers of reassignments, Texas,
Kentucky, and Michigan, and in the sample as a whole, in each of the
two study years, yielding a total of eight models.
Each model was a logistic regression: a binary indicator for section
reassignment was regressed on a random intercept for unit, as well as
nested random intercepts for student, classroom, and school.
Models fit to data from all six states included an additional random
intercept for state.

Logistic regression can be represented in terms of an underlying
latent variable $Z^*$: student $i$ working section $sec$ is reassigned
when $Z_{sec,i}^*>0$.
The model for $Z^*$ is:
\begin{equation*}
 Z^*_{sec,i}=\alpha_0+\beta_{u[sec]}+\gamma_i+\delta_{c[i]}+\epsilon_{s[i]}+e_{sec,i}
\end{equation*}
Where $\alpha_0$ is an overall intercept,
and $\beta_{u[sec]}$, $\gamma_i$, $\delta_{c[i]}$, and $\epsilon_{s[i]}$
are random intercepts for the unit in which $sec$ appears, for student
$i$, for $i$'s classroom, and for $i$'s school, respectively.
Again, the model fit to all six states includes an additional random
intercept for state.
The random intercepts are modeled as independent and normally
distributed, each with its own variance.
The regression error $e_{sec,i}$ is given the standard logistic
distribution, with ``residual'' variance $\pi/3$.
It is convenient to represent variance in reassignment probabilities
in terms of the variance of $Z^*$.

Figure \ref{fig:vc} gives the variance components estimated from these
logistic regressions: variances of the random intercept terms, as a
percentage of the total variance of $Z^*$.
Overall, in both years of the study, the largest determinant of
reassignment was school, accounting for
\Sexpr{vcs[['Overall_1']]['schoolid','sig2']}\% of the variation in year 1,
and \Sexpr{vcs[['Overall_2']]['schoolid',1]}\% in year 2.
After school, state was the most important, accounting for
\Sexpr{vcs[['Overall_1']]['state',1]} and
\Sexpr{vcs[['Overall_2']]['state',1]}\% in the two years, and unit,
accounting for \Sexpr{vcs[['Overall_1']]['unit',1]} and \Sexpr{vcs[['Overall_2']]['unit',1]}\%.
Surprisingly, classroom and student-level factors only accounted for
\Sexpr{vcs[['Overall_1']]['classid',1]} and
\Sexpr{vcs[['Overall_1']]['field_id',1]}\% in year 1, respectively,
and \Sexpr{vcs[['Overall_2']]['classid',1]} and
\Sexpr{vcs[['Overall_2']]['field_id',1]}\% in year 2.
The pattern was similar in Texas, where school accounted for over half
the variation in reassignment in both years, and in Michigan.
In Kentucky, unit played the largest role
(\Sexpr{vcs[['KY_1']]['unit',1]}\%) in year 1, and classroom played
the largest role in year 2 (\Sexpr{vcs[['KY_2']]['classid',1]}\%).
Across states and years, student level factors never accounted for
more than \Sexpr{max(sapply(vcs,function(x) x['field_id',1]))}\% of
the variation in reassignment.
Other than in Kentucky in year 2, classroom never accounted for more
than \Sexpr{sort(sapply(vcs,function(x) x['classid',1]),dec=T)}\%  of
the variation.

Although teachers control reassignment, their decisions are largely
dictated by broader policies, ocurring at the state or school level.


\subsection{When are Students Reassigned?}

The timing of reassignments can provide a window into what drives
teachers' decisions to reassign students.
Figure \ref{fig:byMonth} shows the proportion of worked sections in
each month that end in reassignment.
In both years, reassignments were much more common in the second half
of the school year than in the first.
This may be the result of teachers learning how to use the software as
the year progresses, or responding the pressure of upcoming
standardized tests by accelerating students' progress and reassigning
students to relevent sections.

As we've seen, reassignment was more common in year 2 than in year 1.
In fact, reassignment increases fairly steadily over the entire length
of the study.
Through December of the first year, reassignment was rare, and from
January through May of year 1, between one and two percent of sections
ended in reassignment.
Year 2 begin where year 1 left off, with one to two percent of
sections reassigned.
Finally, from February through May of the second year, the rate of
reassignment incrased again.



\begin{figure}
  \centering
<<byMonth,dependson='cpDat',fig.height=3,fig.width=6>>=
secLevMonth <- secLev%>%group_by(endMonth,Year)%>%
 summarize(nsec=n(),CPper=mean(status=='changed placement',na.rm=TRUE))
secLev$cp <- as.numeric(secLev$status=='changed placement')

ggplot(filter(secLevMonth,nsec>100),aes(endMonth,CPper,group=Year,color=Year,size=nsec))+geom_point()+geom_line(size=1)+
    ## geom_smooth(aes(as.numeric(endMonth)+day(endDate)/31-0.5,cp,group=Year,
    ##                 color=Year,size=1),
    ##             method = "glm", formula = y ~ splines::bs(x, 4),data=secLev,method.args=list(family='binomial'),
                                        #            show.legend=FALSE)+
scale_y_continuous(breaks=seq(0,0.05,.01),labels=percent)+
    coord_cartesian(ylim=c(0,0.05))+
    labs(x='Month',y='% of Sections Ending in Reassignment',size='# Worked \\Sections')


@
\caption{The proportion of worked sections ending in reassignment, by
  month and year.}
\label{fig:byMonth}
\end{figure}


\begin{table}
  \begin{tabular}{rll|ll}
    &\multicolumn{2}{c}{Year 1}&\multicolumn{2}{c}{Year 2}\\
    &Aug--Dec&Jan--Jun&Aug--Dec&Jan--Jun\\
<<byMonthTab,results='asis'>>=

tab <- NULL
for(st in c('TX','KY','MI','LA','CT','NJ')){
 tab <- rbind(tab,
  c(sum(secLev$status[secLev$state==st & secLev$Yr=='Yr 1'&
     secLev$endMonth%in%c('Aug', 'Sep', 'Oct', 'Nov', 'Dec')]=='changed placement'),
    sum(secLev$status[secLev$state==st & secLev$Yr=='Yr 1'&
     secLev$endMonth%in%c('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun')]=='changed placement'),
    sum(secLev$status[secLev$state==st & secLev$Yr=='Yr 2'&
     secLev$endMonth%in%c('Aug', 'Sep', 'Oct', 'Nov', 'Dec')]=='changed placement'),
    sum(secLev$status[secLev$state==st & secLev$Yr=='Yr 2'&
     secLev$endMonth%in%c('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun')]=='changed placement')))
}
tab <- rbind(tab,
    c(sum(secLev$status[secLev$Yr=='Yr 1'&
     secLev$endMonth%in%c('Aug', 'Sep', 'Oct', 'Nov', 'Dec')]=='changed placement'),
    sum(secLev$status[ secLev$Yr=='Yr 1'&
     secLev$endMonth%in%c('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun')]=='changed placement'),
    sum(secLev$status[ secLev$Yr=='Yr 2'&
     secLev$endMonth%in%c('Aug', 'Sep', 'Oct', 'Nov', 'Dec')]=='changed placement'),
    sum(secLev$status[ secLev$Yr=='Yr 2'&
     secLev$endMonth%in%c('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun')]=='changed placement')))
rownames(tab) <- c('TX','KY','MI','LA','CT','NJ','Overall')

print(xtable(tab),only.contents=TRUE,include.colnames=FALSE,hline.after=6)
@
\hline
\end{tabular}
\caption{The number of reassignments in each state and overall in the
  first and second halves of the year}
\label{tab:byMonth}
\end{table}

Figure \ref{fig:byMonthState} and Table \ref{tab:byMonth} decompose
these trends by state, Figure \ref{fig:byMonthState} for
the three states with the highest number of reassignments, Texas,
Kentucky, and Michigan, and Table \ref{tab:byMonth} for all six states
and overall.
For the two study years, Figure \ref{fig:byMonthState} shows the proportion of all of each
state's reassignments the occurred in each month of the school year.
(Note that while both Figure \ref{fig:byMonth} and \ref{fig:byMonth}
show proportions for each month, the denominators are not the same:
Figure \ref{fig:byMonth} gives the proportion of each month's worked
sections that ended in reassignment, and Figure \ref{fig:byMonthState}
gives the proportion of all of each state's reassignmens over the
course of the year, that occurred in each month.)

The pattern in Figure \ref{fig:byMonth} seems largely driven by Texas.
In contrast, the bulk of Kentucky's reassignments took place in May.
A fifth of Michigan's reassignments in year 1 occurred in October;
for the rest of the year, Michigan roughly followed the same rough pattern as Texas.
In year 2, the vast majority of Michigan's reassignments occurred at
the beginning of the year, in September and October.


\begin{figure}
  \centering
<<byMonthState,dependson='cpDat',fig.height=3,fig.width=6>>=

secLevMonthSt <- secLev%>%filter(status=='changed placement'&state%in%c('TX','KY','MI') & endMonth!='Jul')%>%
 group_by(endMonth,Year,state)%>%summarize(cpPer=n())
secLevNum <- secLev%>%filter(status=='changed placement'&state%in%c('TX','KY','MI'))%>%
 group_by(Year,state)%>%summarize(ncp=n())

for(st in c('TX','KY','MI')) for(yr in c('Year 1','Year 2'))
 secLevMonthSt$cpPer[secLevMonthSt$state==st & secLevMonthSt$Year==yr] <-
  secLevMonthSt$cpPer[secLevMonthSt$state==st & secLevMonthSt$Year==yr]/
   secLevNum$ncp[secLevNum$state==st & secLevNum$Year==yr]


## secLevMonthSt <- secLev%>%filter(state%in%c('TX','KY','MI') & endMonth!='Jul')%>%group_by(endMonth,state,Year)%>%summarize(nsec=n(),CPper=mean(status=='changed placement',na.rm=TRUE))
ggplot(secLevMonthSt,aes(endMonth,cpPer,group=state,color=state))+geom_point()+facet_grid(Year~.)+
    geom_line(size=1)+
    scale_y_continuous(labels=percent)+
    labs(x='Month',y='% of State\'s Total Reassignments')


@
\caption{The proportion of a state's reassignments that occurred in
  each month of the year.}
\ref{fig:byMonthState}
\end{figure}

\subsection{Does Reassignment Depend on Classmates?}

Student individuality and independence might be the most important
motivating factors behind mastery learning---each student learns at
his or her own pace, and struggles on a unique set of skills.
Students are supposed to move through the CT curriculum independently
of each other.
However, reassignments give teachers the ability to override this
feature, and coordinate students' progress.
Teachers can identify students who are behind their classmates and
reassign them to later sections.
They may also move an entire class together to a particular section or
unit of interest.
To what extent did these and similar considerations drive reassignment
in the CTAI study?


\begin{figure}
<<classmates,dependson='cpDat',fig.height=6,fig.width=7>>=
secLev$unitSectionClass <- paste(secLev$unit,secLev$section,secLev$classid2)
cpDat <- subset(secLev,status=='changed placement' & Curriculum=='Algebra I')
cpClasses <- unique(cpDat$unitSectionClass)
secLevCP <- filter(secLev,unitSectionClass%in%cpClasses)
levels(secLevCP$status) <- list(promGrad=c('promoted','graduated','final_or_incomplete'),cp='changed placement')
secLevCPsplit <- split(secLevCP[,c('field_id','endDate','status')],secLevCP$unitSectionClass)


ncp <- nrow(cpDat)
seqDat <- matrix(NA,nrow=ncp,ncol=6)
for(i in 1:ncp){
    if(i%%100==0) cat(round(i/ncp*100,2),'% ',sep='')
    cls <- secLevCPsplit[[cpDat$unitSectionClass[i]]]
    if(nrow(cls)==1){
        seqDat[i,] <- 0
        next
    }
    cls <- filter(cls,field_id!=cpDat$field_id[i])
    date <- cpDat$endDate[i]
    ord <- factor(ifelse(cls$endDate<date,'bf',
                  ifelse(cls$endDate==date,'same','after')),levels=c('bf','same','after'))
    clsSplit <- split(cls,list(ord,cls$status))
    seqDat[i,] <- vapply(clsSplit,nrow,1)
}
cat('\n')
colnames(seqDat) <- names(clsSplit)

classSize <- data%>%filter(!is.na(section))%>%group_by(classid2)%>%summarize(nstud=n_distinct(field_id))
cpDat$totClassmates <- classSize$nstud[match(cpDat$classid2,classSize$classid2)]

for(nn in names(clsSplit)) cpDat[[nn]] <- seqDat[,nn]/cpDat$totClassmates
cpDat$nw <- (cpDat$totClassmates-rowSums(seqDat))/cpDat$totClassmates
cpDat$total <- rowSums(seqDat)
cpDat$bf.promGrad <- cpDat$bf.promGrad+cpDat$same.promGrad
cpDat$same.promGrad <- NULL

### plot them all

mmm <- with(subset(cpDat,totClassmates>15),max(table(state,Year)))

cpDat2 <- cpDat%>%filter(totClassmates>15)%>%arrange(bf.promGrad,bf.cp,same.cp,after.cp,after.promGrad)%>%group_by(Year,state)%>%mutate(cpid=seq(floor((mmm-n())/2)+1,floor((mmm-n())/2)+n()))%>%ungroup()

aaa <- melt(cpDat2,measure.vars=c('bf.promGrad','bf.cp','same.cp','after.cp','after.promGrad','nw'))
levels(aaa$variable) <- list(
    `Never Worked\n Section`='nw',
    `Mastered/Promoted\n Later`='after.promGrad',
    `Reassigned\n Later`='after.cp',
    `Reassigned\n Same Day`='same.cp',
    `Reassigned\n Before`='bf.cp',
    `Mastered/Promoted\n Before`='bf.promGrad')

behind <- with(cpDat2,mean(bf.promGrad+bf.cp+same.cp>.75))
behind1 <- with(subset(cpDat2,Yr=='Yr 1'),mean(bf.promGrad+bf.cp+same.cp>.75))
behind2 <- with(subset(cpDat2,Yr=='Yr 2'),mean(bf.promGrad+bf.cp+same.cp>.75))

behind1tx <- with(subset(cpDat2,Yr=='Yr 1'&state=='TX'),mean(bf.promGrad+bf.cp+same.cp>.75))
behind1ky <- with(subset(cpDat2,Yr=='Yr 1'&state=='KY'),mean(bf.promGrad+bf.cp+same.cp>.75))
behind1mi <- with(subset(cpDat2,Yr=='Yr 1'&state=='MI'),mean(bf.promGrad+bf.cp+same.cp>.75))

behind2tx <- with(subset(cpDat2,Yr=='Yr 2'&state=='TX'),mean(bf.promGrad+bf.cp+same.cp>.75))
behind2ky <- with(subset(cpDat2,Yr=='Yr 2'&state=='KY'),mean(bf.promGrad+bf.cp+same.cp>.75))
behind2mi <- with(subset(cpDat2,Yr=='Yr 2'&state=='MI'),mean(bf.promGrad+bf.cp+same.cp>.75))

mi2cp <- round(with(subset(cpDat2,Yr=='Yr 2'&state=='MI'),mean(bf.cp+same.cp+after.cp+nw>.75))*100)
ky1cp <- round(with(subset(cpDat2,Yr=='Yr 1'&state=='KY'),mean(bf.cp+same.cp+after.cp+nw>.75))*100)

#aaa$variable <- factor(aaa$variable,levels=rev(c('pGrad','pProm','pCPbf','pCPsame','pLater','pNW')))
ggplot(filter(aaa,state%in%c('TX','KY','MI')),aes(as.factor(cpid),value,fill=variable))+geom_col()+facet_grid(Year~state)+theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) + scale_y_continuous(label=percent)+labs(fill='% of Classmates')+
    scale_fill_manual(values=c('#1b9e77','#d95f02','#7570b3','#e7298a','#66a61e','#e6ab02'))+
    labs(y='')

@
\caption{Distributions of classmates' statuses at each reassignment in
  Texas, Kentucky, and Michigan. Each reassignment that took place in
  these three states is represented by a bar showing the proportions of
  the reassigned student's classmates who had exited the section on
  the same day or earlier via promotion or mastery, who had been
  reassigned from that section on an earlier date, on the same date,
  or on a later date, who mastered the section or were promoted on a
  later date, or who never worked the section at all. The bars are
  ordered according to those proportions, in the order listed.}
\label{fig:classmates}
\end{figure}

Figure \ref{fig:classmates} addresses this question by plotting a
students' classmates' statuses at the time he or she is reassigned.
For each reassignment in Texas, Kentucky, and Michigan, Figure \ref{fig:classmates}
plots a vertical bar colored to show the proportions of the reassigned
students' classmates (represented in the usage data) who  had exited the section on
the same day or earlier via promotion or mastery, who had been
reassigned from that section on an earlier date, on the same date,
or on a later date, who mastered the section or were promoted on a
later date, or who never worked the section at all.
The bars are ordered according to those same proportions: first by the
proportion of classmates who were promoted or mastered the section on
the same day as the reassignment in question, or earlier, next by the
proportion who were reassigned from the same section on an earlier
date, next by the proportion who were reassigned from the same section
on the same date, and finally by the proportion who were reassigned
from the same section on a later date.

Do teachers reassign students in order to help them catch up with classmates?
According to Figure \ref{fig:classmates}, that might be part of the
story, but isn't all of it.
Across years and states, in about \Sexpr{round(behind*100)}\% of
reassignments, at least 75\% of the rest of the class had exited the
section on the same date or earlier.
In fact, smaller proportions of students had graduated or been
promoted from the same section on the same day or earlier in year 2
than in year 1, especially in Texas and Michigan.

In Texas, year 2 saw a dramatic increase in the proportions of
students reassigned from the same section on the same day, suggesting
that some teachers may have been moving the class together through the
curriculum.
In Michigan in year 2, teachers reassigned almost all students who
worked certain sections.
That is, \Sexpr{mi2cp}\% of the instances in which year-two Michigan
students were reassigned from a section, at least 75\% of their
classmates were, at some point, reassigned from the same section, or
never worked it.
Accross all three states and both years, it is exceedingly rare for
students to master or be promoted from a section after someone in
their class has been reassigned from the same section.

All in all, three different patterns emerge from Figure
\ref{fig:classmates}: teachers reassigning students who have fallen
behind their classmates, teachers reassigning an entire class from the
same section on the same day, and teachers reassigning almost all
students who begin to work on particular sections.
Each of these patterns takes place in different states and
years.

\subsection{Where To?}

Teachers who reassign students may simply move them to the next
section within the same unit.
Say that a teacher believes that a particular student had already mastered
the skills in one of the CTAI sections or is wheel-spinning---working
problems without learning---or dislikes one of the sections in a CT unit.
Still, the teacher wants the student to learn as much as possible from
the student's current unit.
Then moving the student to the next section within the same unit would
make sense.
Alternatively, teachers who believe that some of their students are
not progressing quickly enough may reassign them out of their current
units entirely, and into the next unit in the sequence---whatever that
may be.
Finally, a teacher who wanted his or her students to focus on a
particular topic might reassign them all to the appropriate section
when the time is appropriate.

Which of these patterns is most prevelant?
More generally, when teachers reassign their students, where in the
curriculum do they send them?



<<secOrder,include=FALSE,dependson=c('data','setup')>>=
#### what's the next section each student works?
secOrder <- data%>%filter(is.finite(status) & is.finite(timestamp))%>%group_by(field_id,section,unit,Curriculum,overall,year,Year,Yr,state,classid2,schoolid2)%>%summarize(time=max(timestamp),status=max(status))%>%arrange(time)%>%group_by(field_id,year,Year,Yr,state,classid2,schoolid2)%>%mutate(prevSec=c(NA,section[-n()]),prevStatus=c(NA,status[-n()]),prevUnit=c(NA,unit[-n()]),nextUnit=c(unit[-1],NA))

secOrder$cp <- secOrder$status=='changed placement'
secOrder$mast <- secOrder$status=='graduated'


secOrderCP <- subset(secOrder,status=='changed placement')


secOrderCP$Unit <- secOrderCP$unit
secOrderCP$Unit[grep('unit-conversions',secOrderCP$Unit)] <- 'unit-conversions'

secOrderCP$unitName <- factor(secOrderCP$Unit,levels=units)
levels(secOrderCP$unitName) <- UnitName

secOrderCP$nextUnit[grep('unit-conversions',secOrderCP$nextUnit)] <- 'unit-conversions'

secOrderCP$nextUnitName <- factor(secOrderCP$nextUnit,levels=units)
levels(secOrderCP$nextUnitName) <- UnitName

@



\begin{figure}
  \begin{subfigure}{5in}
<<transition1,fig.height=3,fig.width=5,dependson='secOrder'>>=

trans1 <- with(filter(secOrderCP,year==1),table(unitName,nextUnitName))
trans1o <- trans1

## top senders and receivers
cols <- which(colSums(trans1)>30)
rows <- which(rowSums(trans1)>30)

cols2 <- cols
rows2 <- rows

## top 2 receivers for each sender, vice versa
for(r in rows)
    while(sum(trans1[r,cols2])/sum(trans1[r,])<0.75){
    nextBest <- which.max(trans1[r,seq(ncol(trans1))[-cols2]])
    cols2 <- c(cols2,which(colnames(trans1)==names(nextBest)))
}

for(cc in cols){
    while(sum(trans1[rows2,cc])/sum(trans1[,cc])<0.75){
        nextBest <- which.max(trans1[seq(nrow(trans1))[-rows2],cc])
        rows2 <- c(rows2,which(rownames(trans1)==names(nextBest)))
    }

}

cols <- sort(unique(c(cols2,rows2)))
rows <- sort(unique(rows2))

trans1 <- trans1[rows,cols]

propTotCP1 <- sum(trans1)/sum(secOrderCP$year==1)

uuu <- levels(secOrderCP$unitName)
uuu <- uuu[uuu%in%c(colnames(trans1),rownames(trans1))]
nsec <- length(uuu)

par(mar=c(.1,.1,.1,.1))
plot(1,1,xlim=c(-.5,3.5),ylim=c(0,nsec+2),cex=0,xaxt='n',yaxt='n',xlab='',ylab='')
text(1,nsec+1.5,expression(underline(bold(paste(From)))),adj=c(1,NA))
text(2,nsec+1.5,expression(underline(bold(paste(To)))),adj=c(0,NA))

for(i in 1:nsec) if(uuu[i]%in%rownames(trans1))
  text(1,nsec-i+1,paste0(uuu[i],' (',which(levels(secOrderCP$unitName)==uuu[i]),')'),adj=c(1,NA),cex=0.75)
for(i in 1:nsec) if(uuu[i]%in%colnames(trans1))
  text(2,nsec-i+1,paste0(uuu[i],' (',which(levels(secOrderCP$unitName)==uuu[i]),')'),adj=c(0,NA),cex=0.75)

for(rr in rownames(trans1))
 for(cc in colnames(trans1)){
#     if(trans1[rr,cc]>20){
      arrows(1.1,nsec-which(uuu==rr)+1,1.9,nsec-which(uuu==cc)+1,lwd=5*trans1[rr,cc]/sum(trans1o[rr,]),
       col=rgb(0,0,0,alpha=ifelse(trans1[rr,cc]>50,1,trans1[rr,cc]/50)))
}


@
\end{subfigure}
\begin{subfigure}{1in}
<<trans1legend,fig.width=1,fig.height=3,dependson='transition1'>>=
par(mar=c(.1,.1,.1,.1))
plot(1,1,xlim=c(0,2),ylim=c(0,nsec+2),cex=0,xaxt='n',yaxt='n',xlab='',ylab='')
conv <- (nsec+2)/11
text(1,nsec+1.5,expression(underline(bold(paste(Legend)))))
text(1,9*conv,'Thickness:\n Percent',cex=0.75)
text(.5,7.5*conv,'25%')
text(.5,6.5*conv,'50%')
text(.5,5.5*conv,'75%')
arrows(c(1,1,1),c(7.5,6.5,5.5)*conv,c(2,2,2),lwd=5*c(.25,.5,.75))
text(1,4.2*conv,'Darkness:\n Number',cex=0.75)
text(.5,3*conv,'10')
text(.5,2*conv,'25')
text(.5,1*conv,'50+')
arrows(c(1,1,1),c(3,2,1)*conv,c(2,2,2),col=rgb(0,0,0,alpha=c(1/5,.5,1)))

@
\end{subfigure}
\caption{Reassignment transition plot for study year 1. The units on
  the left of the plot are the Algebra I units that ended in reassignment at
  least 30 times in year 1. The units on the right are those that were
  the destination of at least 30 reassignments. Also included on the
  left are the top two sending units for each of the units on the
  right, and also included on the right are all the units on the left,
  along with their top two receiving units. The units are numbered
  according their order in the standard Algebra I curriculum. There is an arrow from a
  unit on the left to a unit on the right if a student was assigned
  from the unit on the left to the one on the right. The thickness of
  the arrows is proportional to the percentage of all reassignments from
  the sending unit that ended in that receiving unit. The darkness of
  the arrows is proportional to the number of reassignments from the
  sending unit to the receiving unit.}
\label{fig:trans1}
\end{figure}


\begin{figure}
  \begin{subfigure}{5in}
<<transition2,fig.height=5,fig.width=5,dependson='secOrder'>>=

trans2 <- with(filter(secOrderCP,year==2),table(unitName,nextUnitName))
trans2o <- trans2

## top senders and receivers
cols <- which(colSums(trans2)>30)
rows <- which(rowSums(trans2)>30)

cols2 <- cols
rows2 <- rows

## top 2 receivers for each sender, vice versa
for(r in rows)
    while(sum(trans2[r,cols2])/sum(trans2[r,])<0.75){
    nextBest <- which.max(trans2[r,seq(ncol(trans2))[-cols2]])
    cols2 <- c(cols2,which(colnames(trans2)==names(nextBest)))
}

for(cc in cols){
 #   print('new col')
  #  print(cc)
    while(sum(trans2[rows2,cc])/sum(trans2[,cc])<0.75){
        nextBest <- which.max(trans2[seq(nrow(trans2))[-rows2],cc])
        rows2 <- c(rows2,which(rownames(trans2)==names(nextBest)))
    }

}

cols <- sort(unique(c(cols2,rows2)))
rows <- sort(unique(rows2))

trans2 <- trans2[rows,cols]


propTotCP2 <- sum(trans2)/sum(secOrderCP$year==1)

nsec <- nrow(trans2)
uuu <- levels(secOrderCP$unitName)
uuu <- uuu[uuu%in%c(colnames(trans2),rownames(trans2))]
nsec <- length(uuu)

par(mar=c(.1,.1,.1,.1))
plot(1,1,xlim=c(-.5,3.5),ylim=c(0,nsec+2),cex=0,xaxt='n',yaxt='n',xlab='',ylab='')
text(1,nsec+1.5,expression(underline(bold(paste(From)))),adj=c(1,NA))
text(2,nsec+1.5,expression(underline(bold(paste(To)))),adj=c(0,NA))

for(i in 1:nsec) if(uuu[i]%in%rownames(trans2))
  text(1,nsec-i+1,paste0(uuu[i],' (',which(levels(secOrderCP$unitName)==uuu[i]),')'),adj=c(1,NA),cex=0.75)
for(i in 1:nsec) if(uuu[i]%in%colnames(trans2))
  text(2,nsec-i+1,paste0(uuu[i],' (',which(levels(secOrderCP$unitName)==uuu[i]),')'),adj=c(0,NA),cex=0.75)

for(rr in rownames(trans2))
 for(cc in colnames(trans2)){
#     if(trans2[rr,cc]>20){
      arrows(1.1,nsec-which(uuu==rr)+1,1.9,nsec-which(uuu==cc)+1,lwd=5*trans2[rr,cc]/sum(trans2o[rr,]),
       col=rgb(0,0,0,alpha=ifelse(trans2[rr,cc]>50,1,trans2[rr,cc]/50)))
}
@
\end{subfigure}
\begin{subfigure}{1in}
<<trans2legend,fig.width=1,fig.height=3,dependson='transition2'>>=
par(mar=c(.1,.1,.1,.1))
plot(1,1,xlim=c(0,2),ylim=c(0,nsec+2),cex=0,xaxt='n',yaxt='n',xlab='',ylab='')
conv <- (nsec+2)/11
text(1,nsec+1.5,expression(underline(bold(paste(Legend)))))
text(1,9*conv,'Thickness:\n Percent',cex=0.75)
text(.5,7.5*conv,'25%')
text(.5,6.5*conv,'50%')
text(.5,5.5*conv,'75%')
arrows(c(1,1,1),c(7.5,6.5,5.5)*conv,c(2,2,2),lwd=5*c(.25,.5,.75))
text(1,4.2*conv,'Darkness:\n Number',cex=0.75)
text(.5,3*conv,'10')
text(.5,2*conv,'25')
text(.5,1*conv,'50+')
arrows(c(1,1,1),c(3,2,1)*conv,c(2,2,2),col=rgb(0,0,0,alpha=c(1/5,.5,1)),lwd=2.5)

@
\end{subfigure}
\caption{Reassignment transition plot for study year 2.}
\label{fig:trans2}
\end{figure}

To address these questions, we focus on the units in the standard
Algebra I sequence.
Figures \ref{fig:trans1} and \ref{fig:trans2} give transition plots
for reassignment in the two study years.
On the left of each figure, marked ``From,'' are the top ``sending''
units: the units that students were reassigned \emph{from} at least 30
times.
They are numbered according to the order they appear in the standard Algebra I curriculum.
On the right, under ``To,'' are the top ``receiving'' units: the units
that students were reassigned \emph{to} at least 30 times.
For completeness, each of the top two receivers for each sending unit,
and each of the top two senders for each receiving unit were also included.
Finally, all of the sending units were also listed on the left, in the
receiving column.
All in all, \Sexpr{round(propTotCP1*100)}\% of the first-year reassignments and
\Sexpr{round(propTotCP1*100)}\% of the second-year reassignments are
captured in the figures.

The arrows in the plot represent reassignments.
An arrow from a sending unit to a receiving unit indicates
reassignment from the former to the latter.
The thickness of the arrows represents the proportion of reassignments
originating in the sending unit whose destination was the receiving unit.
The darkness of the arrows represents the number of such reassignments.
For instance, in Figure \ref{fig:trans1}, the arrow from
``4-Quadrant Linear Graphs'' on the left to ``4-Quadrant Linear
Graphs'' on the right is fairly thick, since
\Sexpr{round(trans1o['4-Quadrant Linear Graphs','4-Quadrant Linear Graphs']/
  sum(trans1o['4-Quadrant Linear Graphs',])*100)}\%
of the reassignments begun in ``4-Quadrant Linear Graphs'' end in the
same unit.
Yet it is also fairly faint, since it only represents
\Sexpr{trans1o['4-Quadrant Linear Graphs','4-Quadrant Linear Graphs']}
reassignments.

Inspecting the figures shows that the most common pattern is for
students to be reassigned to the next unit in the curriculum---this pattern comprises 
\Sexpr{round(sum(diag(trans1o[,-1]))/sum(trans1o)*100)}\% of the reassignments in year 1 and 
\Sexpr{round(sum(diag(trans2o[,-1]))/sum(trans2o)*100)}\% of those in year2.
On the other hand, it is rare for students to be reassigned within the same unit 
(these reassignments account for \Sexpr{round(sum(diag(trans1o))/sum(trans1o)*100)}\% and \Sexpr{round(sum(diag(trans2o))/sum(trans2o)*100)}\% in the two years).
It is also rare for students to be reassigned to units earlier in the curriculum (comprising 
\Sexpr{round(sum(trans1o[lower.tri(trans1o)])/sum(trans1o)*100)}\% and
\Sexpr{round(sum(trans2o[lower.tri(trans2o)])/sum(trans2o)*100)}\%).

The transition plots also reveal some interesting cases worth highlighting.
In year 1 (Figure \ref{fig:trans1}), students reassigned from the first unit, ``Linear Patterns'', were primarily placed two units ahead, in ``1st Quadrant Linear Graphs,'' skipping ``Unit Conversions,'' perhaps suggesting a disinterest in ``Unit Conversions.'' on the part of the reassigning teachers. 
Similarly, 
\Sexpr{trans1o['Ind. Variables in Lin. Mod.','Systems of Lin. Eq.']} students 
(\Sexpr{round(trans1o['Ind. Variables in Lin. Mod.','Systems of Lin. Eq.']/sum(trans1o['Ind. Variables in Lin. Mod.',])*100)}\%)
of the students reassigned from the section ``Ind. Variables in Lin. Mod.'' were placed 13 units later in ``Systems of Lin. Eq.''
and all \Sexpr{trans1o['Quad. Mod. & Area','Exponents']} of students reassigned from ``Quad. Mod. \& Area'' were placed in ``Exponents.''
This suggests that some teachers may have considered these units, ``Systems of Lin. Eq.'' and ``Exponents'' to contain particularly important material.

Since the total number of reassignments was higher in year 2, the corresponding plot (Figure \ref{fig:trans2}) is larger and more complex. 
It is also more common for students to be reassigned to units other than the next unit in the sequences.
This may be partly due to the proliferation of customized curricula. 
Two units, ``4-Quadrant Linear Graphs'' and ``Lin. Eq. w/ Var's Both Sides,'' were common destinations from a wide variety of earlier units, suggesting a strong interest in those units. 
The majority (\Sexpr{round(trans2o['Probability','Probability']/sum(trans2o['Probability',])*100)}\%)
of the students reassigned from ``Probability'' were placed in another section of the same unit, perhaps suggesting problems with some of that unit's sections; in fact, all of the students reassigned within the ``Probability'' unit were reassigned from one its first three sections (of seven).
Finally, \Sexpr{trans2['Pythagorean Theorem','Product Rule for Exponents']} 
(\Sexpr{round(trans2['Pythagorean Theorem','Product Rule for Exponents']/sum(trans2o['Pythagorean Theorem',])*100)}\%) of the reassignements from sections in the ``Pythagorean Theorem'' unit ended in the earliear ``Product Rule for Exponents'' sections. 

In sum, the most prevalent pattern was for students to be reassigned to the following unit, suggesting that teachers are mostly interested in helping lagging students progress.
On the other hand, a number of examples of other patterns---students moving within the same unit, or to units out of sequence---appear as well, suggesting that some teachers may be finely manipulating their students' curricula. 



\section{Effects of Reassignment}
The goal of CTAI is to help students learn Algebra, so
the most important questions about reassignment are about its effect on learning.
Although the data from this study came from a randomized trial, it was
CTAI as a whole that was randomized, not individual behaviors within CTAI.
Specifically, student reassignment was not randomized.
Therefore, claims regarding the causal effects of reassignment on
learning require strong untestable assumptions that are unlikely to be
true.
As in all observational studies, this includes the assumption that all
counfounding variables---variables that predict both reassignment and
learning---have been measured well and modeled correctly.
Further complicating matters, although reassignment itself is a
well-defined process, in practice it can take many forms, as we have
endevoured to show.
There is no reason to expect the effects of reassignment to be
the same regardless of wheather the teacher used it to help lagging
students catch up, to allocate time to important topics, or for some
other reason.

All that said, observational estimates of reassignment's average
effects can be valuable, if interpreted cautiously.
In the absence of evidence from randomized trials, observational
studies can help guide intuition, future research, and even---when
combined with other relevent information and theory---practice.

<<effects,include=FALSE,dependson=c('data','setup')>>=
cpDat <- data%>%filter(!is.na(status)& status!='final_or_incomplete')%>%
 group_by(field_id,year,state,section,unit,race,sex,grade,spec_speced,xirt,spec_gifted,
  spec_esl,frl,pretest,y_yirt,classid2,schoolid2)%>%
 summarize(status=max(status,na.rm=TRUE),date=max(date),totalTime=sum(total_t1,na.rm=TRUE),
  ttNA=sum(is.na(total_t1)),nprob=n(),probNA=sum(is.na(Prob1)))%>%ungroup()%>%
 group_by(field_id,year,state,race,sex,grade,spec_speced,xirt,spec_gifted,spec_esl,frl,pretest,y_yirt,
  classid2,schoolid2)%>%
 summarize(nsec=n(),nprob=sum(nprob,na.rm=TRUE),totalTime=sum(totalTime,na.rm=TRUE),
  timeNA=sum(ttNA>0),probNA=sum(probNA>0),ncp=sum(status=='changed placement'),
  nprom=sum(status=='promoted'))%>%mutate(gainScore=y_yirt-pretest)%>%droplevels()



## mode imputation:
for(vv in c('race','sex','grade','spec_speced','spec_gifted','spec_esl','frl')){
    num <- is.numeric(cpDat[[vv]])
    cpDat[[paste0(vv,'MIS')]] <- is.na(cpDat[[vv]])
    cpDat[[vv]][is.na(cpDat[[vv]])] <- names(which.max(table(cpDat[[vv]])))
    if(num) cpDat[[vv]] <- as.numeric(cpDat[[vv]])

}

cpDat$grade <- factor(ifelse(cpDat$grade==9,'9','10+'))
levels(cpDat$race) <- list(White=c('WHITE NON-HISPANIC','ASIAN / PACIFIC ISLANDER'),Black=c('BLACK NON-HISPANIC','OTHER RACE / MULTI-RACIAL'),Hispanic=c('HISPANIC','AMERICAN INDIAN / ALASKAN NATIVE'))

cpDat <- mutate(cpDat,totalTime=totalTime/3600000,totalTime=ifelse(totalTime<0,NA,totalTime),
                totalTime=ifelse(totalTime>360,NA,totalTime))

cpDat$ncpCat <- factor(ifelse(cpDat$ncp>=4,'4+',cpDat$ncp))
cpDat$everCP <- cpDat$ncp>0

exirt <- data[match(cpDat$field_id,data$field_id),grep('Exirt2',names(data))]
names(exirt) <- gsub('_0','_',names(exirt))
for(n in names(exirt)) cpDat[[n]] <- exirt[[n]]

@
<<models,include=FALSE,dependson='effects'>>=
#### models!!
## ind var: ncp
addCovs <- .~.+pretest+race+sex+grade+spec_speced+spec_gifted+spec_esl+frl+frlMIS
mod1 <- lm(y_yirt~ncp+classid2,data=cpDat)
mod1.1 <- update(mod1,addCovs)

## ind var: ncpCat
mod2 <- lm(y_yirt~ncpCat+classid2,data=cpDat)
mod2.1 <- update(mod2,addCovs)

## ind var: everCP
mod3 <- lm(y_yirt~everCP+classid2,data=cpDat)
mod3.1 <- update(mod3,addCovs)


miMod <- function(mod){
  if(inherits(mod,'lm')) CCC <- coef
  if(inherits(mod,'lmerMod')) CCC <- fixef
    ests <- NULL
    covs <- list()
    for(i in 1:20){
        formNew <- as.formula(paste0('.~.-pretest+Exirt2_',i))
        modNew <- update(mod,formNew)
        ests <- cbind(ests,CCC(modNew))
        covs[[i]] <- vcov(modNew)
    }
    return(list(ests=ests,covs=covs))
}
miPool <- function(fitMods){
    ests <- fitMods$ests
    covs <- fitMods$covs
    interest <- grep('CP',rownames(ests),ignore.case=TRUE)
    SEs <- sqrt(rowMeans(do.call('cbind',lapply(covs,function(cc) diag(as.matrix(cc[interest,interest])))))+
                1.05*apply(rbind(ests[interest,]),1,var))
    ests <- rowMeans(rbind(ests[interest,]))
    cbind(ests,SEs)
}

mi <- function(mod){
    miPool(miMod(mod))
}

adjustedEsts1 <- list()
for(i in 1:3){
  mm <- paste0('mod',i)
  mm1 <- paste0('mod',i,'.1')
  adjustedEsts1[[mm]] <- mi(get(mm))
  adjustedEsts1[[mm1]] <- mi(get(mm1))
}
@
<<heterogeneityMods,include=FALSE,dependson='models'>>=
### heterogeneity
## year
cpDat$Year=as.factor(cpDat$year)
mod4 <- lmer(y_yirt~everCP*Year+state+(1|classid2)+(1|schoolid2),data=cpDat)
mod4.1 <- update(mod4,addCovs)

## classroom
mod5 <- lmer(y_yirt~everCP+year+state+(everCP|classid2)+(1|schoolid2),data=cpDat)
mod5.1 <- update(mod5,addCovs)
@

\begin{figure}
<<cpYyear,fig.width=6,fig.height=4,dependson='effects'>>=
levels(cpDat$Year) <- list(`Year 1`='1',`Year 2`='2')
ggplot(cpDat,aes(ncp,gainScore))+geom_jitter(height = 0,width=.3)+geom_boxplot(aes(group=ncp),alpha=0.5)+geom_smooth(method='loess')+facet_grid(Year~.)+labs(x='# of Reassignments',y='Gain Score (posttest - pretest)') + scale_x_continuous(breaks=seq(0,max(cpDat$ncp)))
@
\caption{Students' Gain scores (post-test minus pre-test) versus the number of times the were reassigned over the course of the year (jittered), with a Loess smoother added.}
\label{fig:cpYyear}
\end{figure}

\subsection{Average Effects}
Figure \ref{fig:cpYyear} shows students gain scores---the difference between their post-test and pre-test scores---as a function of the number of times they were reassigned.
(The number of reassignments was jittered---random noise was added---to avoid overplotting.)
Overall, the relationship between the two variables is positive.
Nevertheless, some non-linearity seems to be present, especially in year 2.
Further, the distribution of the number of reassignments is right-skewed---again, especially in year 2. 
Care in modeling the number of reassignments, then, is especially important---observations from students reassigned an unusually large number of times can exert undo influence on a regression model and generate misleading results, particularly in the presence of non-monotonic relationships. 
We settled on three different strategies: first, the variable $R^{bin}$ dichotomizes reassignment---$R^{bin}=0$ for students who were never reassigned, and $R^{bin}=1$ for stundents who were. 
Next, $R^{cat}$ defines a categorical variable taking the values $R^{cat}=0,1,2,3,4+$ for students who were reassigned 0, 1, 2, 3 or four or more times, respectively. 
Finally, $R^{num}$ is the raw number of reassignments, which we include for completeness. 

We used linear models to estimate the effect of reassignment, regressing post-test scores on $R^{bin}$, $R^{cat}$, or $R^{num}$ along with fixed effects for classroom, essentially modeling reassignment as randomly assigned within classroom.
Since this is unlikely to be the case (even approximately), we ran a second set of models including student level covariates: 
pretest scores,\footnote{These are measured with error, and missing at a relatively high rate (
\Sexpr{round(mean(is.na(cpDat$xirt))*100)}\%). To account for this, we used regression calibration based on the 20 ``multiple imputations'' used in the original CTAI study, \citet{pane2013effectiveness}.} race, sex, grade, special education and gifted status, English as a second language, and free and reduced-price lunch eligability.


\begin{table}
\begin{tabular}{r|c|c|c|}
&\multicolumn{3}{c}{Parametrization}\\
&$R^{bin}$&$R^{cat}$&$R^{num}$\\
&\makecell[c]{Effect of\\ $\ge 1$ Reassignment}&\makecell[l]{Effect of\\  \# Reassignments:}& \makecell[c]{Effect per\\ Reassignment:}\\
\hline
<<effectResults,results='asis',dependson='models'>>=
## model 3
cat('\\makecell[r]{No\\\\Covariates}&')
cat(round(adjustedEsts1[['mod3']][1,'ests'],2),'$\\pm$',round(2*adjustedEsts1[['mod3']][1,'SEs'],2),'&')  
## model 2
res <- adjustedEsts1[['mod2']]
cat('\\makecell[l]{')
for(rr in 1:(nrow(res)-1)) 
  cat(rr,':  ',round(res[rr,1],2),'$\\pm$',round(2*res[rr,'SEs'],2),'\\\\')
cat(nrow(res),'+: ',round(res[nrow(res),1],2),'$\\pm$',round(2*res[nrow(res),'SEs'],2),'}&')
## model 1
    cat(round(adjustedEsts1[['mod1']][1,'ests'],2),'$\\pm$',round(2*adjustedEsts1[['mod1']][1,'SEs'],2),'\\\\')
cat('\n')
cat('\\hline')
###################################
cat('\\makecell[r]{Covariate\\\\ Adjusted}&')
cat(    round(adjustedEsts1[['mod3.1']][1,'ests'],2),'$\\pm$',round(2*adjustedEsts1[['mod3.1']][1,'SEs'],2),'&')
## model 2
cat('\\makecell[l]{')
res <- adjustedEsts1[['mod2.1']]
for(rr in 1:(nrow(res)-1)) cat(rr,': ',round(res[rr,1],2),'$\\pm$',round(2*res[rr,'SEs'],2),'\\\\')
cat(nrow(res),'+: ',round(res[nrow(res),1],2),'$\\pm$',round(2*res[nrow(res),'SEs'],2),'}&')
## model 1
cat(
    round(adjustedEsts1[['mod1.1']][1,'ests'],2),'$\\pm$',round(2*adjustedEsts1[['mod1.1']][1,'SEs'],2),'\\\\')

@
\hline
\end{tabular}
\caption{Estimates of effects of reassignment on posttests, with 95\% margins of error.}
\label{effectResults}
\end{table}

The results are reported in Table \ref{effectResults}.
All the effect estimates are negative, indicating that reassigning students may hurt their algebra learning.
The estimated effects decrease in magnitude for three or four reassignments, but these estimates are very noisy---very few students were reassigned more than two times. 
The magnitudes of the effects are rather large: \citet{pane2013effectiveness} reported an effect, in year 2, of about 0.2; the estimated effect of at least one reassignment is \Sexpr{round(adjustedEsts1[['mod3.1']][1,'ests']*100)}\% of that.

But what of unmeasured confounding?
\citet{hhh} suggest a method of estimating the sensitivity of a regression to an omitted confounder based on benchmarking from observed confounders.
In order to confound the causal relationship between reassignment and post-tests, a confounder would have to predict both.
Roughly speaking, the idea is to widen the confidence interval from a causal linear model to account for the possibility of a hypothetical unmeasured confounder that predicts reassignment and posttests to the same extent as one of the observed covariates.
These ``sensitivity intervals'' account for uncertainty from two sources: random error, and systematic error due to the omssion of a confounder.
As is typical, the pretest is our most important measured covariate, both in terms of its prediction of reassignment and of post-test scores.
The sensitivity interval for the effect of being reassigned at least once on post-test scores, 
\begin{table}
\begin{tabular}{rrl}
Benchmark&\multicolumn{2}{c}{Sensitivity Interval}\\
<<sens,dependson='models'>>=
source('hhh.r')
X <- as.data.frame(model.matrix(mod3.1)[,-1])
Ts <- Tz(X = X,treatment = 'everCPTRUE')
Rhos <- vapply(names(X)[-grep('classid2',names(X))],
               function(nn) partialCor(cpDat$y_yirt,X,nn),1)[-1]^2
se <- adjustedEsts1[['mod3.1']][1,2]
cat(names(Rhos)[1],'&',round(adjustedEsts1[['mod3.1']][1,'ests'],2),'&$\\pm$',
    round(se*MEmult(abs(Ts[names(Rhos)[1]]),R = Rhos[1],df = mod3.1$df.residual,1.96),2),'\\\\')
for(i in 2:length(Rhos))
  cat(gsub('_','',names(Rhos)[i]),'&&$\\pm$',round(se*MEmult(T=abs(Ts[names(Rhos)[i]]),R = Rhos[i],df = mod3.1$df.residual,1.96),2),'\\\\')
@
\end{tabular}
\caption{Sensitivity intervals for the $R^{bin}$ model.}
\label{tab:sens}
\end{table}



\section{Notes}
[[don't forget to include these]]

\begin{itemize}
  \item \# of students per state?
  \item NJ excluded from state-by-state comparisons, cuz too
    small. Included in ``overall,'' etc.
  \item What schools included?
  \item Students with no mastery data: maybe they didn't work at all,
    maybe they worked but we don't have their data. Treating them as unknown.
\end{itemize}

\end{document}
<<eval=FALSE>>=

 nextDat <- secOrder%>%filter(!is.na(status))%>%group_by(field_id,classid2,year,state)%>%
  summarize(naf=sum(is.na(prevStatus) & status=='final_or_incomplete')/sum(is.na(prevStatus)),
   nacp=sum(is.na(prevStatus) & status=='changed placement')/sum(is.na(prevStatus)),
   naprom=sum(is.na(prevStatus) & status=='promoted')/sum(is.na(prevStatus)),
   nagrad=sum(is.na(prevStatus) & status=='graduated')/sum(is.na(prevStatus)),
   ff=sum(prevStatus==1 & status=='final_or_incomplete',na.rm=TRUE)/sum(prevStatus==1,na.rm=TRUE),
   fcp=sum(prevStatus==1 & status=='changed placement',na.rm=TRUE)/sum(prevStatus==1,na.rm=TRUE),
   fprom=sum(prevStatus==1 & status=='promoted',na.rm=TRUE)/sum(prevStatus==1,na.rm=TRUE),
   fgrad=sum(prevStatus==1 & status=='graduated',na.rm=TRUE)/sum(prevStatus==1,na.rm=TRUE),
   cpf=sum(prevStatus==2 & status=='final_or_incomplete',na.rm=TRUE)/sum(prevStatus==2,na.rm=TRUE),
   cpcp=sum(prevStatus==2 & status=='changed placement',na.rm=TRUE)/sum(prevStatus==2,na.rm=TRUE),
   cpprom=sum(prevStatus==2 & status=='promoted',na.rm=TRUE)/sum(prevStatus==2,na.rm=TRUE),
   cpgrad=sum(prevStatus==2 & status=='graduated',na.rm=TRUE)/sum(prevStatus==2,na.rm=TRUE),
   promf=sum(prevStatus==3 & status=='final_or_incomplete',na.rm=TRUE)/sum(prevStatus==3,na.rm=TRUE),
   promcp=sum(prevStatus==3 & status=='changed placement',na.rm=TRUE)/sum(prevStatus==3,na.rm=TRUE),
   promprom=sum(prevStatus==3 & status=='promoted',na.rm=TRUE)/sum(prevStatus==3,na.rm=TRUE),
   promgrad=sum(prevStatus==3 & status=='graduated',na.rm=TRUE)/sum(prevStatus==3,na.rm=TRUE),
   gradf=sum(prevStatus==4 & status=='final_or_incomplete',na.rm=TRUE)/sum(prevStatus==4,na.rm=TRUE),
   gradcp=sum(prevStatus==4 & status=='changed placement',na.rm=TRUE)/sum(prevStatus==4,na.rm=TRUE),
   gradprom=sum(prevStatus==4 & status=='promoted',na.rm=TRUE)/sum(prevStatus==4,na.rm=TRUE),
   gradgrad=sum(prevStatus==4 & status=='graduated',na.rm=TRUE)/sum(prevStatus==4,na.rm=TRUE),
   npna=sum(is.na(prevStatus)),npf=sum(prevStatus==1,na.rm=TRUE),npcp=sum(prevStatus==2,na.rm=TRUE),
    npprom=sum(prevStatus==3,na.rm=TRUE),npgrad=sum(prevStatus==4,na.rm=TRUE))

@

