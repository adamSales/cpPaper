\documentclass[12pt]{article}

%\usepackage{endfloat}
%\usepackage{soul}

%\usepackage{type1ec}
\usepackage{subcaption}
\usepackage{fullpage}
\usepackage{enumerate}
%\usepackage{graphicx}
%\usepackage{graphics}
\usepackage{multirow}
\usepackage{comment}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{bbm}
\usepackage{setspace}
\usepackage{verbatim}
\usepackage{natbib}
\usepackage{bm}
\usepackage{pdflscape}
%\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage{xr}
%\usepackage{mathabx}
%\usepackage{filecontents}
%\usepackage{bibentry}
%\usepackage{hanging}
%\usepackage{apacite}
%\usepackage{hyperref}
\usepackage{makecell}

%\usepackage{endnotes}

%\let\footnote=\endnote


\title{Mastery Learning in Practice:\\
A Descriptive Analysis of Data from the  Cognitive Tutor Algebra I Effectiveness Trial}

\author{Anita Israni \and Adam C Sales \and John F Pane}


<<include=FALSE>>=
library(knitr)
#library(tikzDevice)
opts_chunk$set(
echo=FALSE, results='asis',cache=TRUE,warning=FALSE,error=FALSE,message=FALSE,autodep = FALSE
    )

#options(tikzDefaultEngine = "pdftex")
@

<<setup,include=FALSE>>=

library(lubridate)
library(scales)
library(reshape2)
library(dplyr)
library(ggplot2)
library(xtable)

addOverallState <- function(dat){
    levels(dat$state) <- c(levels(dat$state),'Overall')
    dat2 <- dat
    dat2$state <- 'Overall'
    dat2 <- rbind(dat,dat2)
    dat2$state <- factor(dat2$state, levels= levels(dat$state))
    dat2
}

@


<<data,include=FALSE>>=
if(file.info('cpPaper.RData')$mtime>file.info('dataMerge.r')$mtime){
    load('cpPaper.RData')
} else source('dataMerge.r')
@

<<data1,include=FALSE,cache=FALSE>>=
dataCache <- grep('data',list.files('cache'),ignore.case=FALSE,value=TRUE)[1]
if(file.info('dataMerge.r')$mtime>file.info(paste0('cache/',dataCache))$mtime)
    source('dataMerge.r')
@


\begin{document}
\maketitle

\section{Introduction}
Mastery learning sits at the foundation of intelligent tutoring systems. The philosophy of mastery learning assumes a well-structured curriculum, and posits that students progress within the curriculum as they master its skills. The Cognitive Tutor Algebra I (CTAI) system, developed by Carnegie Learning, Inc., is one of the best studied and best regarded examples of modern educational software. It is a blended learning system for teaching algebraic concepts and principles, to middle and high school students, including both textbook materials and software. The software component of the curriculum allows students to progress at their own pace and receive individualized feedback on their performance. A large-scale randomized effectiveness trial conducted by the RAND corporation showed that, in some circumstances, CTAI boosts students’ scores on an Algebra-I post-test by about one fifth of a standard deviation (Pane, et al.  2013). CTAI’s success in this experiment would seem to vindicate its pedagogy: mastery learning, and the algebra I curriculum on which it is based.

However, the theory underlying CTAI does not always determine its use. To be sure, the software has a standard set of algebra topics, divided into units and further into sections; and a standard sequence for presenting them. But this precise curriculum is not mandatory. At the request of educators, it can be customized by altering what units or sections are included (including, possibly, material from a different standard curriculum such as geometry), as well as their sequence, to conform to local or state standards. Further, teachers have the option of moving students within the curriculum, regardless of the software’s estimate of their skill mastery.

This article examines teachers’ and schools’ non-adherence to the standard, mastery-based CTAI curriculum, using data from the RAND study, a seven-state randomized controlled trial of CTAI in high schools and middle schools. The study found a significant positive effect of CTAI in high schools, during their second year of implementation but not the first. Students in the treatment group of the study were enrolled in one or more of the standard or customized curriculum during their participation in the study, and the software logged aspects of their usage, including time spent, sections encountered, and whether the software judged the students to have mastered the sections. Both Carnegie Learning and the researchers running the study restricted their software support and oversight to what is typically provided outside of an experimental context. Thus, the data from the study reflect typical software usage. Secondary data analyses used principal stratification to show that students who attempted more sections experienced larger treatment effects, and students who had high or low assistance levels, as opposed to an average level, experienced smaller treatment effects (Sales \& Pane, 2015, Sales et al. 2016).

Here we extend the previous findings to investigate patterns of change in the sequence of topics presented to students. As noted earlier, schools could use a standard curriculum, a customized curriculum, or switch from one curriculum to another. Each curriculum consisted of a sequence of units, which was comprised of a sequence of sections. For example, the standard CTAI Algebra I curriculum in Year 1 comprised of 40 units covering approximately 170 sections. The
number of sections within each unit varied.
This study attempts to elucidate teachers' goals in reassigning students. One hypothesis is the need for teachers push ahead students who were falling behind, i.e. to reassign them from sections on which they were struggling, to allow them catch up with the rest of the class. Another hypothesis is that teachers sought to push students past easier sections to begin working on more relevant topics for them. A third hypothesis is that teachers needed to cover certain topics in preparation for an upcoming state exams, and might have reassigned groups of students all at the same time to cover topics that might otherwise not have been covered. This hypothesis may lead to an increase in reassignments as the exam approaches. Overall, understanding patterns of reassignment is the focus of this paper, along with insights into how teachers were using the software component of CTAI.

\section{The RAND Effectiveness Trial}
The study to measure the effectiveness of CTAI included 7 states, 73 high schools, and 74 middle schools with nearly 18,700 high school students and 6,800 middle school students. Schools were enrolled in a total of 52 school districts that were distributed between urban, suburban, and rural areas. Schools were matched on a set of covariates, and then randomly assigned to the treatment or control group. Schools in the control group continued with their current algebra curriculum, and schools in the treatment group used Carnegie Learning’s curriculum which includes textbook materials as well as CTAI. Each school participated for two years, with a different cohort of students taking algebra the second year (with a small fraction of students present in the study both years because they repeated algebra). It should be noted that this study did not include statewide implementations; the study results cannot be generalized to all schools within the state. In some states, one large school district participated, while in other states, a set of smaller school districts participated. The states included Alabama (AL), Connecticut (CT), Kentucky (KY), Louisiana (LA), Michigan (MI), New Jersey (NJ), and Texas (TX). Each state participated in both the middle school and high school arms of the study, except AL, which participated only in the middle school arm. The current study focuses on high school students only, due to the large number of students in this cohort and the significant positive treatment effect previously found for this cohort. All discussion that follows refers exclusively to the high school data.

There are some limitations to the available data for this study. In particular, the developer was able to provide the intended sequence of units/sections for only a subset of the customized curricula. Data from some schools, and some students within schools, were missing either because the log files were not retrievable, or because of an imperfect ability to link log data to other study data files. In addition, there was missing completion dates for many customized curricula; making it difficult to analyze the customized curricula when any sort of date was needed, such as time to complete a section.

\section{Standard and Customized Curricula}

\begin{figure}
  \centering
<<curricula,dependson=c('setup','data'),fig.width=6.4,fig.height=3>>=

bbb <- addOverallState(data)%>%filter(!is.na(Curriculum) & State!='NJ')%>%
    group_by(state,Curriculum,overall,Yr)%>%
    summarize(n=n())
staten <- addOverallState(data)%>%filter(!is.na(Curriculum) & State!='NJ')%>%group_by(state,Yr)%>%summarize(n=n())

for(i in 1:nrow(bbb)) bbb$n[i] <- bbb$n[i]/staten$n[staten$state==bbb$state[i] & staten$Yr==bbb$Yr[i]]

ggplot(bbb,aes(Yr,n,fill=Curriculum,alpha=overall))+geom_col()+facet_grid(~state)+scale_alpha_manual(values=c(1,0.5))+scale_fill_manual(values=COLS)+scale_y_continuous(labels=percent)+labs(x='',y='% of Problems Worked',alpha='')

## ggplot(bbb,aes(Yr,n,color=bbb))+geom_col(show.legend=FALSE)+geom_col(aes(fill=Curriculum))+geom_col(aes(fill=overall),alpha=0.5)+facet_grid(~state)
##     scale_fill_manual(values=c('#8dd3c7','#8dd3c7','#ffffb3','#ffffb3','#bebada','white','black'))
##     geom_col(aes(Yr,n,fill=overall),alpha=0.5)+
##     #geom_col(aes(Yr,n,fill=overall))+#scale_fill_manual(values=c('white','black'))+
##     facet_grid(~state)

## omar <- par()$oma

## par(oma=c(0,2,0,0),mar=c( 3,.5,2,0))
## par(mfrow=c(1,9))
## for(st in levels(data$State)){
## #    cwy <- currWorkedSt%>%filter(State==st)%>%group_by(Curriculum,overall)%>%summarize(`Yr 1`=p[Year=='Year 1'],`Yr 2`=p[Year=='Year 2'])
##     cwy <- data%>%filter(!is.na(Curriculum) & State==st)%>%group_by(Curriculum,overall)%>%
##     summarize(`Yr 1`=sum(Year=='Year 1'),`Yr 2`=sum(Year=='Year 2'))
##     for(y in c(1,2)) cwy[,paste('Yr',y)] <- cwy[,paste('Yr',y)]/sum(cwy[,paste('Yr',y)])
##     cwy <- cwy[nrow(cwy):1,]
##     barplot(as.matrix(cwy[,3:4]),col=COLS[as.numeric(cwy$Curriculum)],angle=ifelse(cwy$overall=='Customized',45,NA),density=ifelse(cwy$overall=='Customized',50,NA),main=st,yaxt='n')
##     if(st==levels(data$State)[1]) axis(2,at=seq(0,1,0.1),labels=paste0(seq(0,100,10),'\\%'))
##     #box()
## }
## cwy <- data%>%filter(!is.na(Curriculum))%>%group_by(Curriculum,overall)%>%
##     summarize(`Yr 1`=sum(Year=='Year 1'),`Yr 2`=sum(Year=='Year 2'))
## for(y in c(1,2)) cwy[,paste('Yr',y)] <- cwy[,paste('Yr',y)]/sum(cwy[,paste('Yr',y)])
## cwy <- cwy[nrow(cwy):1,]
## barplot(as.matrix(cwy[,3:4]),col=COLS[as.numeric(cwy$Curriculum)],angle=ifelse(cwy$overall=='Customized',45,NA),density=ifelse(cwy$overall=='Customized',50,NA),main='Overall',yaxt='n')

## plot(1:10,type='n',xaxt='n',yaxt='n',bty='n')
## legend(-1.5,7.5, c("Bridge-to-\n Algebra",'Algebra I','$>$Alg. I','','Standard','Cust.'),
##        fill=c(COLS,NA,'#010101','#010101'),border=rep('white',6),
##        angle=c(NA,NA,NA,NA,NA,45),density=c(NA,NA,NA,NA,NA,50),xpd = TRUE, bty = "n",inset=c(0,0))

## par(oma=omar)
@
\caption{Percentage of worked problems coming from various levels
  (denoted by color, with Algebra II and Geometry bundled as ``$>$Algebra I''), from
  standard and customized variants, denoted by shading.}
\label{fig:curricula}
\end{figure}

Students' automatic progress throught the Cognitive Tutor (CT) software is governed by
curricula---sequences of sections and units embedded in the software.
Without external meddling, the curriculum a student works on
determines what section he or she will be directed towards next after
mastering (or exhausting the problems) from a previous section.
In the CTAI effectiveness trial, the most common curriculum was,
naturally, Algebra I.
This came with three closely related varients, due to software releases.
Students requiring more remediation were able to work on a less
advanced curriculum, called ``Bridge to Algebra,'' and more advanced
students could work on Algebra II or Geometry.

In the second year of the study, some high schools, primarily in
Texas, Michigan, and Kentucky, requested customized variants of the
curricula.
This was typically due to state standards and testing schedules.
These ``customized curricula'' altered the order of some sections and
units, and were particular to schools.

Figure \ref{fig:curricula} shows the percentage of worked problems
from each curriculum, from standard and customized varieties, by state
and year.
First, note that the vast majority of worked problems were from the
Algebra I sequence.
A small but notable number of more advanced problems were worked in
Kentucky in Year 2, and some less advanced problems were worked in
Michigan and Louisiana.
Secondly, note the rise in ``customized curricula'' in year 2 in
Texas, Kentucky, and Michigan, the three states with the most students
in our dataset.
In particular, Texas shifted almost entirely to customized curriclua
from years 1 to 2.


% All students who participated in the treatment condition of the study were enrolled in at least one curriculum throughout the school year.
% Figure
% In Year 1, the standard Algebra curriculum was the most-used curriculum across schools participating in the study; other curricula were Bridge-to-Algebra (pre-algebra), Algebra II, and five customized curricula that were used in two schools. The standard curricula used in Year 2 included the three that were used in Year 1, plus Geometry and Algebra II Bonus. In Year 2, Carnegie Learning also developed many more customized curricula with input from participating schools, for a total of 33 curricula used in 13 schools. This led to fewer students enrolled in the standard curriculum and more enrolled in customized curricula overall in Year 2.

% In the available data, representing a subset of the entire study sample, the number of students enrolled in at least one standard curriculum in Year 1 was 2,801; in Year 2, this number decreased to 1,204. The number of students enrolled in at least one customized curriculum was 115 in Year 1 and increased to 2,333 in Year 2. It should be noted that students could be enrolled in multiple curricula. For instance, teachers could choose to enroll students in any of the standard curricula provided, such as Algebra I or Algebra II, along with any of the customized curricula that were used by the school.

% The use of customized curricula varied substantially by state. For instance, schools in CT and NJ did not use customized curricula in either Year 1 or Year 2. All other states contained schools that used customized curricula in addition to the standard curricula. CT schools maintained similar number of students in Algebra I across both years with 111 in Year 1 and 135 in Year 2. NJ substantially decreased the number of students in the CTAI program from 89 to 32 from Year 1 to Year 2. However, while all other states (KY, LA, MI, and TX) did decrease enrollment in the standard Algebra I curriculum the second year, those same states increased enrollment in the customized curricula. KY added 3 customized curricula with 146 students, LA added 15 customized curricula with 286 students, MI added 5 customized curricula with 370 students, and TX added 10 additional customized curricula and had the largest enrollment in these curricula with 1,531 students. Students enrolled in the standard Algebra I curriculum dropped from 974 in Year 1 to 61 in Year 2 in TX. As noted above, students could be enrolled in more than one curriculum. In Year 2, the average number of curricula per a student ranged from 1.00 (NJ and CT) to 2.53 (TX). The next highest value was LA at 2.10.

Throughout the school year, teachers could have a class of students
working on multiple curricula either sequentially, where the students
changed curricula in lock step, or simultaneously, where students
worked on different curricula at the same time. As an example, two
teachers located in KY had their students working on Algebra I
throughout most of the year and then reassigned them to Algebra II in
the last month of school. Another teacher in KY had students variously
enrolled in three different curricula throughout the entire year
(Bridge-to-Algebra, Algebra I, and a specialized Geometry curriculum),
while another teacher in MI during Year 2 had students enrolled in
three curricula sequentially throughout the year covering Algebra I
until November, and then starting a customized curriculum and using it
until February, and ending the year with another customized curriculum
that was used until June. While there are numerous instances of these
uses of multiple curricula in Year 2, there are also many occurrences
of teachers who had their students enrolled in the standard Algebra I
throughout the entire year.  In TX mostly, with some instances in MI,
there were teachers who exclusively used customized curricula the
whole year.

\section{Student Usage Across States and Years}

<<usageMedians,dependson=c('data','setup'),results='asis'>>=
secByStud <- data%>%group_by(state,Yr,field_id) %>% summarize(numSec=n_distinct(section,na.rm=TRUE),
                                                              numUnit=n_distinct(unit,na.rm=TRUE),
                                                              time=sum(total_t1,na.rm=TRUE),
                                                              mastered=n_distinct(section[status=='graduated'],na.rm=TRUE),
                                                              nprob=n_distinct(unit,section,Prob1,na.rm=TRUE))

secByStud$time <- secByStud$time/3600000

secByStud <- within(secByStud,{
    numUnit[numUnit==0] <- NA
    numSec[numSec==0] <- NA
    time[time==0] <- NA
    mastered[mastered==0] <- NA
})

secByStud2 <- addOverallState(secByStud)

secByStud2 <-
  secByStud2 %>%
  group_by(state,Yr) %>%
  mutate(outlier.time = time > quantile(time,0.75,na.rm=TRUE) + IQR(time,na.rm=TRUE) * 1.5,
         outlier.sec = numSec > quantile(numSec,0.75,na.rm=TRUE) + IQR(numSec,na.rm=TRUE) * 1.5,
         outlier.prob = nprob > quantile(nprob,0.75,na.rm=TRUE) + IQR(nprob,na.rm=TRUE) * 1.5,
         outlier.unit = numUnit > quantile(numUnit,0.75,na.rm=TRUE) + IQR(numUnit,na.rm=TRUE) * 1.5
         ) %>%
  ungroup


tab <- secByStud%>%group_by(Yr)%>%select(time,nprob,numSec,numUnit)%>%summarize_all(median,na.rm=TRUE)
tab$Yr <- NULL
tab <- as.data.frame(tab)
rownames(tab) <- c('Year 1','Year 2')
names(tab) <- c('Hours','Problems','Sections','Units')

xtable(tab,caption='Median numbers of hours, problems, sections, and units worked by each student in the dataset in the two years of the study. Students with no usage data were excluded',label='tab:medUsage',digits=c(1,2,0,1,0))
@


Table \ref{tab:tab:medUsage} shows the median numbers of hours,
problems, sections, and units worked on by each student in the dataset
in the two years of the study.
Apparently, usage decreased markedly in the second year: the median
student worked about
\Sexpr{round(tab['Year 1','Hours']-tab['Year 2','Hours'])}
fewer hours,
\Sexpr{round(tab['Year 1','Problems']-tab['Year 2','Problems'])}
fewer problems, and
\Sexpr{round(tab['Year 1','Sections']-tab['Year 2','Sections'])} fewer
sections in year 2 than in year 1.


\begin{figure}
\centering
<<usageTime, fig.height=3,fig.width=6,dependson=c('data','setup','usageMedians')>>=


print(timeStateYear <- ggplot(filter(secByStud2,state!='NJ'),aes(Yr,time))+geom_boxplot(outlier.shape=NA)+
          geom_jitter(data = function(x) dplyr::filter_(x, ~ outlier.time), width=0.2)+
          facet_grid(~state)+coord_cartesian(ylim=c(0,110))+labs(x='',y='Hours on CT Software per Student'))
                                        #ggsave('timeStatYear.jpg',width=6,height=3)





@
\caption{Boxplots of hours each student spent on Cognitive Tutor
  software over by year and state. Students  with no timestamp data
  (\Sexpr{sum(is.na(secByStud[['time']]))}), with negative time
  (\Sexpr{sum(secByStud[['time']]<0,na.rm=TRUE)}) or with more than 110 hours (\Sexpr{sum(secByStud[['time']]>110,na.rm=TRUE)})
  were excluded.}
\label{fig:timeByStud}
\end{figure}

<<supplementalFigs1,include=FALSE,dependson=c('data','setup','usageMedians')>>=
secStateYear <- ggplot(filter(secByStud2,state!='NJ'),aes(Yr,numSec))+geom_boxplot(outlier.shape=NA)+
          geom_jitter(data = function(x) dplyr::filter_(x, ~ outlier.sec), width=0.2)+
          facet_grid(~state)+labs(x='',y='# of Sections Worked per Student')+coord_cartesian(ylim=c(0,200))
ggsave('secStateYear.jpg',width=6,height=3)

probStateYear <- ggplot(filter(secByStud2,state!='NJ'),aes(Yr,nprob))+geom_boxplot(outlier.shape=NA)+
          geom_jitter(data = function(x) dplyr::filter_(x, ~ outlier.prob), width=0.2)+
          facet_grid(~state)+labs(x='',y='# of Problems Worked per Student')+coord_cartesian(ylim=c(0,2000))
ggsave('probStateYear.jpg',width=6,height=3)

@

% \begin{figure}
% <<mastSec,dependson=c('usageTime','data','setup'),fig.width=6,fig.height=3>>=
% print(mastStateYear <- ggplot(secByStud2,aes(Yr,mastered))+geom_boxplot()+facet_grid(~state)+labs(x='',y='\\# Sections Mastered Per Student')+coord_cartesian(ylim=c(0,250)))
% @
% \caption{Boxplots of the number of sections each of Cognitive Tutor
%   software each student mastered, by year and state. Students
%   mastering more than 250 sections (\Sexpr{sum(secByStud[['mastered']]>250,na.rm=TRUE)/2}) of \Sexpr{nrow(secByStud)/2}) and students
%   with no mastery data (\Sexpr{sum(is.na(secByStud[['mastered']]))/2})
%   were excluded.}
% \label{fig:mastSec}
% \end{figure}



Figure \ref{fig:timeByStud} shows that the number of hours students
spent working on the CT software in some more detail, via
state-by-year boxplots.
Analogous figures for the numbers of problems and sections students
worked, available in an online supplement, showed similar patterns for
the numbers of sections and problems students worked.
Usage time varied substantially between students and across states and
years.
Time spent varied substantially by state, with students in Texas,
Connecticut, and New Jersey working far fewer hours than students in
Kentucky, Louisiana, and Michigan.
Not every state reduced its usage from years 1 to 2---while students
in Texas, Kentucky and New Jersey used the software less in the second
year than in the first, students in Michigan, Louisiana, and
Connectuct increased their usage.

Overall, usage varied a bit more in year 2 than in year
1---the median absolute deviation of time spent was \Sexpr{round(mad(secByStud[['time']][secByStud[['Yr']]=='Yr 1'],na.rm=TRUE,constant=1),1)} hours in the first year, compared to \Sexpr{round(mad(secByStud[['time']][secByStud[['Yr']]=='Yr 2'],na.rm=TRUE,constant=1),1)} in  the second year.
The increase in variation seems to be driven both by increasing
between-state variation, and a between-student increase in Louisiana.
One intriguing possibility is that CT usage was better tailored to
teachers and students in the second year than in the first---perhaps
students who stood to gain more from the software increased their
usage, and the rest of the students used it less.

In contrast to the numbers of hours, problems, and sections students
worked, Table \ref{tab:medUsage} shows that the median number of units
students worked increased by
\Sexpr{round(tab['Year 2','Units']-tab['Year 1','Units'])}
from year 1 to year 2.
This suggests students in year 2 were exposed, on average, to a
slightly wider range of topics.
Figure \ref{fig:unitsByStud} shows boxplots of the numbers of units
worked by state and year.
The geographic variation in units worked mirrors the pattern in Figure
\ref{fig:timeByStud}, with more usage in Kentudky, Michigan, and
Louisiana but less in Texas, Connecticut, and New Jersey.
However, in every state the median Year 2 student worked at least as
many different units as the median Year 1 student.
Variation in the number of units worked also increased slightly
from years 1 to 2---the interquartile range increased in every state
except for Kentucky, where a decrease in IQR was accompanied by an
incrase in the number of outliers.



\begin{figure}
\centering
<<unitsWorked,dependson=c('usageMedians','data','setup'),fig.width=6,fig.height=3>>=
secByStud2 <-
  secByStud2 %>%
  group_by(state,Yr) %>%
  mutate(outlier = numUnit > quantile(numUnit,0.75,na.rm=TRUE) + IQR(numUnit,na.rm=TRUE) * 1.5) %>%
  ungroup

print(unitsStateYear <- ggplot(filter(secByStud2,state!='NJ'),aes(Yr,numUnit))+geom_boxplot(outlier.shape=NA)+geom_jitter(data = function(x) dplyr::filter_(x, ~ outlier), width=0.2)+facet_grid(~state)+labs(x='',y='# Units Worked Per Student')+coord_cartesian(ylim=c(0,55)))
@
\caption{Boxplots of the number of units of Cognitive Tutor
  software each student worked, by year and state. Students
  working more than 55 units (\Sexpr{sum(secByStud[['numUnit']]>55,na.rm=TRUE)}) of \Sexpr{nrow(secByStud)}) and students
  with no usage data (\Sexpr{sum(is.na(secByStud[['numUnit']]))})
  were excluded.}
\label{fig:unitsByStud}
\end{figure}

All in all, students used CT software less in year 2 than in year
1.
On the other hand, students in the second year tended to see a
slightly wider range of topics, and varied somewhat more in their usage.

\begin{comment}
Of course, more worked sections does not necessarily mean better worked sections. For instance, students might attempt fewer sections in year two, but graduate a higher percentage. Thus we investigated whether, on average, there were more graduated sections per students the second year, and how this related to whether they were enrolled in the standard versus customized curricula. Overall, the number of graduated sections per student increased in Year 2. The number of graduated sections per student was 17.94 in Year 1 and 25.66 in Year 2 in standard curricula. However, the customized curricula had fewer average graduated sections per student enrolled compared to the standard curricula; these numbers were 5.64 in Year 1 and 11.47 in Year 2. The percentages of average graduated sections to the average total worked sections were: 33.33% in Year 1 and 48.92% in Year 2 in the standard curricula, and 57.55% in Year 2 for the customized curricula. The average number of promoted sections per student also increased from Year 1 to Year 2, though only slightly. In Year 1, the average number of promotions were 1.20 for the customized curricula versus 3.64 for the standard curricula; in Year 2, those numbers were 2.20 and 4.95, respectively.

We also evaluated the time spent per section across different curricula and states.  Some records were omitted from this analysis: sections missing information on date of completion, as well as each student’s last section attempted because there is no completion date recorded for those sections. As presented in Figures 2 and 3 for Year 1 and 2, respectively, students in NJ increased the time spent per section from Year 1 to Year 2; however, there were relatively few students in the study from NJ. From Figure 1, it was shown that CT and TX completed, on average, fewer sections per student. From the information presented in Figures 2 and 3, these states spent the highest average number of days per section. TX, with the largest enrollment of students and the greatest use of customized curricula in Year 2, had students spending slightly more days completing sections in customized curricula relative to the standard curricula.

Figures 4 and 5 present information for only sections that were “graduated” by students, that is, cases in which a student was judged by the software to have mastered the material in the section. Results are similar to the data presented in Figures 2 and 3. However, in Year 2, TX students in customized curricula spent fewer days on a section they mastered, compared to the time TX students spent mastering sections in the standard curricula. This difference is also seen in KY and LA. If customized curriculum are more aligned with teachers’ instruction, graduating from a section may require fewer days than sections in the standard curricula, possibly because students are working on the same topics in class as they are in the software. Figures 4 and 5 also show that, on average, CT and TX spent more days graduating from sections, regardless whether the section was part of a standard curricula or customized curricula. One explanation for the longer time periods in these states may be due to a teacher’s use of the software simultaneously with the textbook instruction. In this case, the teachers could have switched back-and-forth as they instructed, hence extending the time worked by their students. For the states with shorter time periods, teachers may have followed a more sequential process where they first completed one component, e.g. textbook material, and then allowed students to focus on and complete the second component, e.g. software.
\end{comment}


\section{Working Units in Order---Or Not}

Overally, students used CT less in the second year than in the
first.
How was this difference distributed between CTAI units?

\begin{figure}
<<whichUnits,fig.height=4,fig.width=6,dependson=c('data','setup')>>=

curricula <- read.csv('~/Box Sync/CT/data/sectionLevelUsageData/RAND_study_curricula.csv',stringsAsFactors=FALSE)
curricula <- subset(curricula,curriculum_name=='algebra i')
curricula$unit <- tolower(curricula$unit)
curricula <- subset(curricula,unit%in%intersect(curricula$unit[curricula$ct=='2007'],curricula$unit[curricula$ct=='2008r1']))
units <- curricula$unit[curricula$ct=='2007']
sectionStats <- read.csv('~/Box Sync/CT/data/sectionLevelUsageData/section_stats_withAbb.csv',stringsAsFactors=FALSE)
UnitName <- sectionStats$unit_name_abb[match(units,sectionStats$unit_id)]
UnitName[units=='inequality-systems-solving'] <- 'Systems of Lin. Ineq.'
UnitName[units=='intro-pythag-theorem'] <- 'Pythagorean Theorem'
UnitName[units=='linear-inequality-graphing'] <- 'Graphs of Lin. Ineq.'
UnitName[units=='linear-systems-solving'] <- 'Systems of Lin. Eq. Solving'
UnitName[units=='probability'] <- 'Probability'
UnitName[units=='unit-conversions'] <- 'Unit Conversions'

nstud <- data%>%filter(!is.na(unit))%>%group_by(Year)%>%summarize(nstud=n_distinct(field_id))
data$Unit <- data$unit
data$Unit[grep('unit-conversions',data$Unit)] <- 'unit-conversions'

unitLevel <- data%>%filter(Unit%in%units)%>%group_by(Unit,Year)%>%summarize(numWorked= n_distinct(field_id,na.rm=TRUE),numCP=sum(status=='changed placement',na.rm=TRUE),meanCP=mean(status=='changed placement',na.rm=TRUE))

unitLevel$perWorked <- unitLevel$numWorked/nstud$nstud[match(unitLevel$Year,nstud$Year)]

unitLevel$Unit <- factor(unitLevel$Unit,levels=units)
levels(unitLevel$Unit) <- UnitName

unitLevel$year <- factor(ifelse(unitLevel$Year=='Year 1',1,2))
print(unitsWorked <- ggplot(unitLevel,aes(x=Unit,y=perWorked,color=year,group=year))+geom_point()+geom_line()+theme(axis.text.x = element_text(angle = 90, hjust = 1,vjust=0.5,size=10),legend.text=element_text(size=10),legend.position=c(.96,.82))+labs(x='',y='% Worked',color='Year')+scale_y_continuous(labels=percent))

@
\caption{The percentages of students with usage data who worked at
  least one problem from each unit in the Algebra I curriculum. The
  units are arranged in order for the standard curriculum. In the 2008
version of the software, the ``Unit Conversions'' unit was broken up
into two smaller units; for the sake of between-year comparisons, we
re-combined them.}
\label{fig:unitsWorked}
\end{figure}


Figure \ref{fig:unitsWorked} shows the percentage of students with
usage data who worked each section from the standard CTAI curriculum.
The units are ordered, from left to right, as the are prescribed in
the standard CTAI curriculum.

In year 1, the curve is almost monontonically decreasing, as one would
expect if students adhered to the curriculum.
Students varied in how many units they worked---with the variation due
to both student ability and the amount of time allocated to CTAI
within a classroom---but they mostly followed the same curriculum.
Students who worked fewer units stopped earlier in the curricular
order, and those who worked more units progressed farther.
Hence, earlier sections were worked by higher proportions of students
than later units.

In contrast, in year 2 students were much more likely to buck the
standard unit order.
For instance, Figure \ref{fig:unitsWorked} suggests that students
skipped ``Unit Conversions'' and ``1 step Lin. Eq.'' for ``1st
Quadrent Linear Graphs'' and ``Ind. Variables in Lin. Mod.''
Both of the latter sections were worked by a greater proportion of
students than the former sections that preceded them in the CTAI
curriculum.
Most strikingly, ``Lin. Eq. w/ Var's Both Sides'' was worked by a
greater proportion of students in Year 2 than in Year 1, and by a
greater proportion of students than any of the previous six sections.
Presumeably teachers and administrators wanted students to focus on
that unit, perhaps because they found it to be particularly effective,
because students tend to struggle with its main topic, or because its
topic may figure prominantly in an upcoming standardized test.

\begin{figure}
  \centering
<<unitsWorkedCust,dependson=c('data','setup','whichUnits'),fig.height=4,fig.width=6>>=
### by customized curriculum (at school level)
cust <- data%>%filter(Year=='Year 2')%>%group_by(schoolid2,state)%>%summarize(cust=mean(overall=='Customized',na.rm=T))%>%arrange(cust)
data$cust <- ifelse(data$schoolid2%in%cust$schoolid2[cust$cust>0.8],'Customized','Standard')

nstudCust <- data%>%filter(!is.na(unit) & Year=='Year 2')%>%group_by(cust)%>%summarize(nstud=n_distinct(field_id))
unitLevelCust <- data%>%filter(Unit%in%units & Year=='Year 2' )%>%group_by(Unit,cust)%>%summarize(numWorked= n_distinct(field_id,na.rm=TRUE),numCP=sum(status=='changed placement',na.rm=TRUE),meanCP=mean(status=='changed placement',na.rm=TRUE))

unitLevelCust$perWorked <- unitLevelCust$numWorked/nstudCust$nstud[match(unitLevelCust$cust,nstudCust$cust)]

unitLevelCust$Unit <- factor(unitLevelCust$Unit,levels=units)
levels(unitLevelCust$Unit) <- UnitName
ggplot(unitLevelCust,aes(x=Unit,y=perWorked,color=cust,group=cust))+geom_point()+geom_line()+theme(axis.text.x = element_text(angle = 90, hjust = 1,vjust=0.5,size=10),legend.text=element_text(size=10),legend.position=c(.96,.82))+labs(x='',y='% Worked',color='Curriculum',title='Year 2')+scale_y_continuous(labels=percent)
@
\caption{The percentages of year-2 students with usage data who worked at
  least one problem from each unit in the Algebra I curriculum. The
  units are arranged in order for the standard curriculum. Students
  are divided between those attending schools using primarily a customized
  curriculum and those using primariy the standard Algebra I curriculum.}
\label{fig:unitsWorkedCust}
\end{figure}

Most of the variation in unit order was driven by the rise, in year 2,
of customized curricula.
Figure \ref{fig:unitsWorkedCust} divides year-2 students into those
attending schools using primarily a customized curriculum, and those
attending schools using primarily a standardized
curriculum.\footnote{At least
  \Sexpr{trunc(100*min(cust[['cust']][cust[['cust']]>0.8]))}\% of
  problems at ``Customized'' schools were from a customized curriculum,
  and at most
  \Sexpr{ceiling(100*max(cust[['cust']][cust[['cust']]<0.8]))}\% of
  problems at ``Standard'' schools were from a customized curriculum.}
Students using a standardized curriculum followed the standard
sequence---more or less---while students using customized curricula
did not.
That said, there were some order violations in the standard group:
specifically, more students worked problems from units ``4-Quadrant
Linear Graphs'' and ``Exponents'' than worked the preceding sections;
this suggests that some teachers used the reassignment tool to
prioritize particular topics.
Of course, this may have occurred in schools with customized
curriculua as well---a possibility we will discuss in the next
section.

\begin{figure}
  \centering
<<unitsBySchool,dependson=c('data','setup','whichUnits'),fig.height=7,fig.width=6>>=
### by school
nstudSch <- data%>%filter(!is.na(unit) & Year=='Year 2')%>%group_by(schoolid2,state)%>%summarize(nstud=n_distinct(field_id))
unitLevelSch <- with(filter(data,Unit%in%units & Year=='Year 2'),expand.grid(Unit=unique(Unit),schoolid2=unique(schoolid2)))
unitLevelSch$state <- data$state[match(unitLevelSch$schoolid2,data$schoolid2)]
unitLevelSch$cust <- data$cust[match(unitLevelSch$schoolid2,data$schoolid2)]
unitLevelSch$numWorked <- with(filter(data,Year=='Year 2'),vapply(1:nrow(unitLevelSch),function(i)
    n_distinct(field_id[schoolid2==unitLevelSch$schoolid2[i] & Unit==unitLevelSch$Unit[i]]),1))

#unitLevelSch <- data%>%filter(Unit%in%units & Year=='Year 2')%>%group_by(Unit,schoolid2,cust,state)%>%summarize(numWorked= n_distinct(field_id,na.rm=TRUE),numCP=sum(status=='changed placement',na.rm=TRUE),meanCP=mean(status=='changed placement',na.rm=TRUE))

unitLevelSch$perWorked <- unitLevelSch$numWorked/nstudSch$nstud[match(unitLevelSch$schoolid2,nstudSch$schoolid2)]

unitLevelSch$Unit <- factor(unitLevelSch$Unit,levels=units)
levels(unitLevelSch$Unit) <- UnitName

ggplot(filter(unitLevelSch,state!='NJ'),aes(x=Unit,y=perWorked,color=schoolid2,group=schoolid2,linetype=cust))+geom_point(size=.5)+geom_line()+theme(axis.text.x = element_text(angle = 90, hjust = 1,vjust=0.5,size=10),legend.text=element_text(size=10))+
                                        #,legend.position=c(.96,.82))+
    facet_grid(state~.)+scale_y_continuous(labels=percent)+scale_color_discrete(guide=FALSE)+
    labs(x='',y='% Worked',linetype='Curriculum',title='Year 2')
@
\caption{The percentages of year-2 students with usage data in each
  school who worked at least one problem from each unit in the Algebra I curriculum. The
  units are arranged in order for the standard curriculum. Schools are
  classified as either using primarily customized curricula (solid
  line) or using primarily the standard Algebra I curriculum (dotted).}
\label{fig:unitsBySchool}
\end{figure}

Figure \ref{fig:unitsBySchool} further decomposes the year-2 results
by school and state, showing a large amount of variation between
states, as well as variation between schools within states.
In Texas, every school used customized curricula, most of which seem to
prioritize some of the same units, for instance, ``Linear Patterns,''
``Ind. Variables in Lin. Mod.,'' and
``Lin. Eq. w/ Var's Both Side.''
These topics might be emphasized on Texas standardized tests.
On the other hand, there was also variance between schools.
For instance, one school prioritized units ``2 Step Lin. Eq.''
and ``4−Quadrant Linear Graphs'' while nearly eliminating ``Linear Patterns.''

Between-school variation is evident in the other states, as well.
In four of the five Kentucky schools, nearly every student worked on
the first nine units; in the one Kentucky school that used a
customized curriculum, nearly every student worked on the first 13
units, omitted the 15th (``Lin. Mod. in General Form''), and worked on
the 16th and 17th (``Literal Eq.'' and ``Lin. Eq. w/ Var's Both Sides'').
In the remaining school, nearly every student worked on the first
section, but usage decreased rapidly from there.
Curiously, in one Michigan school which used the standard curriculum,
no students seem to have worked on the ``Lin. Mod. \& Ratios'' section.

If unit order and topic scaffolding are important to CT's mastery
learning mechanism, the wide variation in students' realized curricula
would seem to pose a problem.
The fact that the prescribed order was followed less in the second
year of the study, when CTAI was effective, than in the first year,
when it wasn't, suggests that the standard curriculum may play a
smaller role than one might otherwise imagine.

\section{Mastering the Material---Or Not}
The central idea behind mastery learning is that students progress
through the curriculum as they master skills.
In the context of CT, skills are clustered within sections, which are
in turn clustered within units.
Students progress from the current section to the next section after
mastering all of the current section's skills.
Ideally, students would master all of the skills in all of the
sections they work.

By default, the software operates by automatically moving students
from section to section based on the sequence of topics defined by the
curriculum they were currently enrolled in. In this
software-controlled sequencing, students ideally spend the time
necessary to learn the material of a section, are judged by the
software to have mastered the material, and then ``graduate'' to the
next section. Students who exhaust a section's material without
mastering its skills are “promoted” to the next section.
Students left uninterrupted follow the sequence of topics defined by
their curriculum, ideally graduating from topics by showing mastery.
However, teachers could modify a student’s path within the curriculum.
They could ``reassign'' students from their current sections to other
sections earlier or later in the intended sequence, including sections
they had worked on previously.
Finally, if the semester ends, or a student stops using CT for some
other reason, while in the middle of working through a
section, he or she will leave their ``Final'' section without
mastering all of its skills.
All in all, each CT section begun ends in one of four possible ways:
mastery, promotion, reassignment, or as the student's final section.

\begin{figure}
  \centering
<<overallStatus,dependson=c('data','setup'),fig.height=3,fig.width=6>>=
statusPerSec <- data%>%
    filter(!is.na(Curriculum) &!(state=='MI'&Curriculum=='Customized'&year==1))%>%
    group_by(field_id,Yr,state,section,Curriculum,status)%>%
        summarize(cp=any(status=='changed placement'))%>%group_by(state,Curriculum,Yr,field_id)%>%summarise(pcp=mean(cp,na.rm=TRUE),ncp=sum(cp,na.rm=TRUE))

statusOverall <-  data%>%filter(!is.na(status))%>%
    group_by(field_id,Yr,state,unit,section,Curriculum,overall)%>%summarize(status=max(status))

statusStateYr <- addOverallState(statusOverall) %>% group_by(state,Yr)%>%
    summarize(pgrad=mean(status=='graduated'),pfoi=mean(status=='final_or_incomplete'),
              pcp=mean(status=='changed placement'),pprom=mean(status=='promoted'))%>%
    melt(measure.vars=c('pgrad','pfoi','pcp','pprom'))

levels(statusStateYr$variable) <- list(Final='pfoi',Reassigned='pcp',Promoted='pprom',Mastered='pgrad')


 ggplot(filter(statusStateYr,state!='NJ'),aes(Yr,value,fill=variable))+geom_col()+facet_grid(~state)+labs(y='% of Sections Worked',x='',fill='Exit Status')+scale_y_continuous(labels=percent)

@
\caption{The distributions of outcomes of worked sections, by state and across the
  entire sample, in the two study years.}
\label{fig:overallStatus}
\end{figure}

\begin{table}
 \begin{tabular}{rllllll|llllll}

&   \multicolumn{6}{c}{Year 1}&\multicolumn{6}{c}{Year 2}\\
<<statusTab,dependson='overallStatus',results='asis'>>=

tab1 <- dcast(subset(statusStateYr,Yr=='Yr 1'&state!='NJ'),variable~state)
tab2 <- dcast(subset(statusStateYr,Yr=='Yr 2'&state!='NJ'),variable~state)
tab <- cbind(tab1,tab2[-1])

tab$variable <- as.character(tab$variable)
cat('&')
cat(names(tab)[-1],sep='&')
cat('\\\\')
for(i in 1:nrow(tab)){
    cat(tab[i,1],round(unlist(tab[i,-1])*100),sep='&')
    cat('\\\\')
}

rownames(tab1) <- rownames(tab2) <- tab1$variable
tab1 <- as.matrix(tab1[,-1])
tab2 <- as.matrix(tab2[,-1])

@

\end{tabular}
\caption{Percentages of worked sections that ended in each of the
  four possible outcomes, across states and study years.}
\label{tab:overallStatus}
\end{table}

Figure \ref{fig:overallStatus} and Table \ref{tab:overallStatus} show
the proportions of worked sections in each state and study year that
ended with mastery, promotion, or reassignment, or which were the
student's final section.
In the first year, about
\Sexpr{round(mean(tab1['Mastered',c('TX','KY','MI','LA')])*100)}\% of
worked sections are mastered in every
state other than Connecticut.
Other than in Texas, about
\Sexpr{paste(round(range(tab1['Promoted',c('CT','KY','MI','LA')])*100),collapse='--')}\%
of sections end in promotion.
About \Sexpr{round(tab1['Reassigned','TX']*100)}
and \Sexpr{round(tab1['Reassigned','CT']*100)}\% of sections in Texas
and Connecticut end in reassignment, which is even rarer in the other states.

With the exception of Texas, sections in the second year tend to be
completed as they were in year 1.
In Texas, however, the percentage of sections ending in reassignment increased by a factor
of about about
\Sexpr{round(tab2['Reassigned','TX']/tab1['Reassigned','TX'])},
to about \Sexpr{round(tab2['Reassigned','TX']*100)}\%.
The proportion of Texas sections labeled ``Final'' increased as well,
most likely due to the decrease in usage.

Across states, just under
\Sexpr{round(tab1['Reassigned','Overall']*100)}\% of sections in year
1 eneded in reassignment, as did roughly
\Sexpr{round(tab2['Reassigned','Overall']*100)}\% in year 2.

\subsection{Section Mastery and Curriculum}
\begin{figure}
<<statusCur,dependson='overallStatus',fig.height=3,fig.width=6>>=
statusCurr <- statusOverall%>%filter(!is.na(Curriculum))%>% group_by(Curriculum,overall)%>%
    summarize(pgrad=mean(status=='graduated'),pfoi=mean(status=='final_or_incomplete'),
              pcp=mean(status=='changed placement'),pprom=mean(status=='promoted'))%>%
    melt(measure.vars=c('pgrad','pfoi','pcp','pprom'))
levels(statusCurr$variable) <- list(Final='pfoi',Reassigned='pcp',Promoted='pprom',Graduated='pgrad')
statusCurr$Curriculum <- factor(statusCurr$Curriculum,levels=rev(levels(statusCurr$Curriculum)))
print(ggplot(statusCurr,aes(overall,value,fill=variable))+geom_col()+facet_grid(~Curriculum)+labs(y='% of Sections Worked',x='',fill='Exit Status')+scale_y_continuous(labels=percent))

@
\caption{The distributions of outcomes of worked sections, by curriculum, in the two study years.}
\label{fig:statusCur}
\end{figure}


\section{Digging Deeper into Section Reassignment}
<<cpDat,dependson=c('data','setup'),include=FALSE>>=
secLev <- data%>%filter(is.finite(status) & is.finite(timestamp) &is.finite(date))%>%
    group_by(field_id,unit,section,Year,Yr,classid2,schoolid2)%>%
    summarize(startDate=min(date),endDate=max(date),startTime=min(timestamp),endTime=max(timestamp),state=state[1],
              status=max(status),Curriculum=Curriculum[1],overall=overall[1],version=version[1])%>%
    arrange(endDate)

### cp over time
secLev <- within(secLev,endMonth <- factor(month(secLev$endDate,TRUE,TRUE),levels=c('Aug','Sep','Oct','Nov','Dec','Jan','Feb','Mar','Apr','May','Jun','Jul')))

@
\subsection{When are Students Reassigned?}

\begin{figure}
  \centering
<<byMonth,dependson='cpDat',fig.height=3,fig.width=6>>=
secLevMonth <- secLev%>%group_by(endMonth,Year)%>%summarize(ncp=n(),CPper=mean(status=='changed placement',na.rm=TRUE))
secLev$cp <- as.numeric(secLev$status=='changed placement')

ggplot(filter(secLevMonth,ncp>100),aes(endMonth,CPper,group=Year,color=Year,size=ncp))+geom_point()+geom_line(size=1)+
    ## geom_smooth(aes(as.numeric(endMonth)+day(endDate)/31-0.5,cp,group=Year,
    ##                 color=Year,size=1),
    ##             method = "glm", formula = y ~ splines::bs(x, 4),data=secLev,method.args=list(family='binomial'),
                                        #            show.legend=FALSE)+
scale_y_continuous(breaks=seq(0,0.05,.01),labels=percent)+
    coord_cartesian(ylim=c(0,0.05))+ labs(x='Month',y='% of Sections Ending in Reassignment')


@
\end{figure}

\begin{figure}
  \centering
<<byMonthState,dependson='cpDat',fig.height=3,fig.width=6>>=
secLevMonthSt <- secLev%>%filter(state%in%c('TX','KY','MI') & endMonth!='Jul')%>%group_by(endMonth,state,Year)%>%summarize(nsec=n(),CPper=mean(status=='changed placement',na.rm=TRUE))
ggplot(secLevMonthSt,aes(endMonth,CPper,group=state,color=state,size=nsec))+geom_point()+facet_grid(Year~.)+
    geom_line(size=1)+
    ## geom_smooth(aes(as.numeric(endMonth)+day(endDate)/31-1.5,cp,group=state,
    ##                 color=state,size=1),
    ##             method = "glm",
    ##             formula = y ~ splines::ns(x, knots=seq(1,10,2)),
    ##             data=filter(secLev,state%in%c('TX','CT','NJ')& endMonth!='Jul'),
    ##             method.args=list(family='binomial'),
    ##             show.legend=FALSE)+
    scale_y_continuous(labels=percent)+
    labs(x='Month',y='% Sections Ending in Reassignment',size='Worked \n Sections',color='')


@
\end{figure}

\subsection{Does Reassignment Depend on Classmates?}
\begin{figure}
<<classmates,dependson='cpDat',fig.height=6,fig.width=7>>=
secLev$unitSectionClass <- paste(secLev$unit,secLev$section,secLev$classid2)
cpDat <- subset(secLev,status=='changed placement' & Curriculum=='Algebra I')
cpClasses <- unique(cpDat$unitSectionClass)
secLevCP <- filter(secLev,unitSectionClass%in%cpClasses)
levels(secLevCP$status) <- list(promGrad=c('promoted','graduated','final_or_incomplete'),cp='changed placement')
secLevCPsplit <- split(secLevCP[,c('field_id','endDate','status')],secLevCP$unitSectionClass)


ncp <- nrow(cpDat)
seqDat <- matrix(NA,nrow=ncp,ncol=6)
for(i in 1:ncp){
    if(i%%100==0) cat(round(i/ncp*100,2),'% ',sep='')
    cls <- secLevCPsplit[[cpDat$unitSectionClass[i]]]
    if(nrow(cls)==1){
        seqDat[i,] <- 0
        next
    }
    cls <- filter(cls,field_id!=cpDat$field_id[i])
    date <- cpDat$endDate[i]
    ord <- factor(ifelse(cls$endDate<date,'bf',
                  ifelse(cls$endDate==date,'same','after')),levels=c('bf','same','after'))
    clsSplit <- split(cls,list(ord,cls$status))
    seqDat[i,] <- vapply(clsSplit,nrow,1)
}
cat('\n')
colnames(seqDat) <- names(clsSplit)

classSize <- data%>%filter(!is.na(section))%>%group_by(classid2)%>%summarize(nstud=n_distinct(field_id))
cpDat$totClassmates <- classSize$nstud[match(cpDat$classid2,classSize$classid2)]

for(nn in names(clsSplit)) cpDat[[nn]] <- seqDat[,nn]/cpDat$totClassmates
cpDat$nw <- (cpDat$totClassmates-rowSums(seqDat))/cpDat$totClassmates
cpDat$total <- rowSums(seqDat)
cpDat$bf.promGrad <- cpDat$bf.promGrad+cpDat$same.promGrad

### plot them all

mmm <- with(subset(cpDat,totClassmates>15),max(table(state,Year)))

cpDat2 <- cpDat%>%filter(totClassmates>15)%>%arrange(bf.promGrad,bf.cp,same.cp,after.cp,after.promGrad)%>%group_by(Year,state)%>%mutate(cpid=seq(floor((mmm-n())/2)+1,floor((mmm-n())/2)+n()))%>%ungroup()

aaa <- melt(cpDat2,measure.vars=c('bf.promGrad','bf.cp','same.cp','after.cp','after.promGrad','nw'))
levels(aaa$variable) <- list(
    `Never Worked\n Section`='nw',
    `Mastered/Promoted\n Later`='after.promGrad',
    `Reassigned\n Later`='after.cp',
    `Reassigned\n Same Day`='same.cp',
    `Reassigned\n Before`='bf.cp',
    `Mastered/Promoted\n Before`='bf.promGrad')





#aaa$variable <- factor(aaa$variable,levels=rev(c('pGrad','pProm','pCPbf','pCPsame','pLater','pNW')))
ggplot(filter(aaa,state%in%c('TX','KY','MI')),aes(as.factor(cpid),value,fill=variable))+geom_col()+facet_grid(Year~state)+theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) + scale_y_continuous(label=percent)+labs(fill='% of Classmates')+
    scale_fill_manual(values=c('#1b9e77','#d95f02','#7570b3','#e7298a','#66a61e','#e6ab02'))

@
\end{figure}

\subsection{Where To?}
\begin{figure}
<<transition,fig.height=7,fig.width=6.5,dependson=c('data','setup')>>=
#### what's the next section each student works?
secOrder <- data%>%filter(is.finite(status) & is.finite(timestamp))%>%group_by(field_id,section,unit,Curriculum,overall,year,Year,Yr,state,classid2,schoolid2)%>%summarize(time=max(timestamp),status=max(status))%>%arrange(time)%>%group_by(field_id,year,Year,Yr,state,classid2,schoolid2)%>%mutate(prevSec=c(NA,section[-n()]),prevStatus=c(NA,status[-n()]),prevUnit=c(NA,unit[-n()]),nextUnit=c(unit[-1],NA))

secOrder$cp <- secOrder$status=='changed placement'
secOrder$mast <- secOrder$status=='graduated'


secOrderCP <- subset(secOrder,status=='changed placement')


secOrderCP$Unit <- secOrderCP$unit
secOrderCP$Unit[grep('unit-conversions',secOrderCP$Unit)] <- 'unit-conversions'

secOrderCP$unitName <- factor(secOrderCP$Unit,levels=units)
levels(secOrderCP$unitName) <- UnitName

secOrderCP$nextUnit[grep('unit-conversions',secOrderCP$nextUnit)] <- 'unit-conversions'

secOrderCP$nextUnitName <- factor(secOrderCP$nextUnit,levels=units)
levels(secOrderCP$nextUnitName) <- UnitName

trans <- with(secOrderCP,table(unitName,nextUnitName))

trans <- trans[rowSums(trans)>50,colSums(trans)>10]
trans <- trans[rowSums(trans)>50,colSums(trans)>10]
trans <- trans[rowSums(trans)>50,colSums(trans)>10]


#transitionPlot(trans)

nsec <- nrow(trans)
uuu <- levels(secOrderCP$unitName)
uuu <- uuu[uuu%in%c(colnames(trans),rownames(trans))]
nsec <- length(uuu)

par(mar=c(.1,.1,.1,.1))
plot(1,1,xlim=c(-.5,4),ylim=c(0,nsec),cex=0,xaxt='n',yaxt='n',xlab='',ylab='')
for(i in 1:nsec) if(uuu[i]%in%rownames(trans)) text(1,nsec-i+1,uuu[i],adj=c(1,NA))
for(i in 1:nsec) if(uuu[i]%in%colnames(trans)) text(2,nsec-i+1,uuu[i],adj=c(0,NA))

for(rr in rownames(trans))
 for(cc in colnames(trans))
     if(trans[rr,cc]>20){
      arrows(1.1,nsec-which(uuu==rr)+1,1.9,nsec-which(uuu==cc)+1,lwd=5*trans[rr,cc]/sum(trans[rr,]),
       col=rgb(0,0,0,alpha=ifelse(trans[rr,cc]>100,1,trans[rr,cc]/100)))
}


@
\end{figure}

\begin{figure}
<<vcs,fig.height=7,fig.width=6.5,dependson=c('data','setup')>>=
load('vcMods.RData')
library(lme4)

vcFun <- function(nm){
    mod <- vcMods[[nm]]
    out <- unlist(summary(mod)$varcor)
    out <- data.frame(sig2=out,comp=names(out),stringsAsFactors=FALSE)
    out <- rbind(out,data.frame(sig2=pi^2/3,comp='resid'))
    out$state <- strsplit(nm,'_')[[1]][1]
    out$year <- strsplit(nm,'_')[[1]][2]
    out$sig2 <- out$sig2/sum(out$sig2)
    out
}

vcDat <- do.call('rbind',lapply(names(vcMods),vcFun))

yr1 <- data.frame(sig2=c(unlist(summary(vcModYr[[1]])$varcor),pi^2/3),
                          comp=c(names(summary(vcModYr[[1]])$varcor),'resid'),
                          state='Overall',
                  year='1')
yr1$sig2 <- yr1$sig2/sum(yr1$sig2)

yr2 <- data.frame(sig2=c(unlist(summary(vcModYr[[2]])$varcor),pi^2/3),
                          comp=c(names(summary(vcModYr[[2]])$varcor),'resid'),
                          state='Overall',
                  year='2')
yr2$sig2 <- yr2$sig2/sum(yr2$sig2)

vcDat <- rbind(vcDat,yr1,yr2)

vcDat$comp <- factor(vcDat$comp)
levels(vcDat$comp)=list(State='state',School='schoolid2',Class='classid2',Student='field_id',Unit='unit',Residual='resid')


vcDat$state <- factor(vcDat$state,levels=c('TX','KY','MI','Overall'))

ggplot(vcDat,aes(year,sig2,fill=comp))+geom_col()+facet_grid(~state)+scale_fill_manual(values=rev(c('white','grey','#e41a1c','#377eb8','#4daf4a','#984ea3')))+labs(x='',y='% Variance Explained',fill='',title='Model: Multilevel Logistic Unconditional')+scale_y_continuous(labels=percent)



@
\end{figure}


\begin{figure}
<<xirtCP,fig.height=7,fig.width=6.5,dependson=c('data','setup')>>=
load('cpXirtMods.RData')

xirtDat <- data%>%filter(!is.na(status) & !is.na(xirt))%>%
    group_by(field_id,classid2,schoolid2,xirt,year,spec_gifted,spec_speced,state,unit,section)%>%
    summarize(status=max(status,na.rm=TRUE),nprob=n())%>%
    group_by(field_id,classid2,schoolid2,xirt,year,spec_gifted,spec_speced,state)%>%
    summarize(nprob=sum(nprob),nsec=n_distinct(section,unit),propMast=mean(status=='graduated'),
              propCP=mean(status=='changed placement'))%>%arrange(state)

xirtDat$fitted <- NA
for(st in names(cpModSt)) xirtDat$fitted[xirtDat$state==st] <-
                                  predict(cpModSt[[st]],xirtDat[xirtDat$state==st,],
                                          allow.new.levels=TRUE,re.form=~0,type='response')


xirtDat <- addOverallState(xirtDat)

xirtDat$fitted[xirtDat$state=='Overall'] <-
    predict(cpModSimp,xirtDat[xirtDat$state=='Overall',],
            allow.new.levels=TRUE,re.form=~0,type='response')

ggplot(subset(xirtDat,state!='NJ'),aes(xirt,propCP,size=nsec))+geom_point()+geom_line(aes(xirt,fitted),col='red',size=1)+facet_grid(year~state)+coord_cartesian(ylim=c(0,0.05))


@
\end{figure}

\section{Notes}
[[don't forget to include these]]

\begin{itemize}
  \item \# of students per state?
  \item NJ excluded from state-by-state comparisons, cuz too
    small. Included in ``overall,'' etc.
  \item What schools included?
  \item Students with no mastery data: maybe they didn't work at all,
    maybe they worked but we don't have their data. Treating them as unknown.
\end{itemize}

\end{document}
